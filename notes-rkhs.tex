
RKHS's are spaces of ``regular'' FUNCTIONS (not equivalence classes in $L^2[0,1]$) with a different norm to that inherited from $L^2[0,1]$. See \sidetextcite{paulsen2016introduction} for a more detailed introduction.

\subsection*{Definition}

	Let $K(s,t)$ be a positive definite (the matrix $K(t_i,t_j)$ is always positive definite) real function on $[0,1]^2$.

	Let $\mathcal H_0(K):=\{f\in L^2[0,1] \ : \ f(\cdot)=\sum_{i=1}^n a_i K(t_i,\cdot),\ a_i\in{\mathbb R},\ t_i\in[0,1],\ n\in{\mathbb N}\}$, be
	the space of all finite linear combinations of evaluations of $K$. This space is endowed with the inner product
	$\langle f,g\rangle_K=\sum_{i,j}\alpha_i\beta_j K(t_i,s_j)$,
	where $f(\cdot)=\sum_i\alpha_i K(t_i,\cdot)$ and $g(\cdot)=\sum_j\beta_j K(s_j,\cdot)$.

	Then, the RKHS associated with $K$ is defined as the completion of $\mathcal H_0(K)$. In other words, $\mathcal H(K)$ is made of all functions obtained as pointwise limits of Cauchy sequences in $\mathcal H_0(K)$. The inner product is extended accordingly to the whole space $\mathcal H(K)$.

\subsection*{Reproducing property}

	These spaces are named after the so-called \textit{reproducing property},
	$\langle f, K(s,\cdot)\rangle_K = f(s)$, for all  $f\in\mathcal H(K), s\in[0,1]$, which is particularly important in the applications. On account of this property it is sometimes said that the RKHS are spaces of ``true functions'', in the sense that the pointwise values $f(s)$ at a given $s$ do matter, in contrast with $L^2[0,1]$ whose elements are in fact equivalence classes of functions.

  Note that in an RKHS the evaluation functional is continuous, so a small perturbation $f\mapsto f + \delta f$ results in a small change in $f(x)$ for all $x$.


\subsection*{Loève's isometry}

	A property of RKHS's especially useful in statistical applications is given by the following \textit{Lo\`eve's isometry}:  let $L^2(\Omega) $ be the Hilbert space of real random variables with finite second moment, endowed with the usual inner product whose associated norm is $\| U\|^2={\mathbb E}(U^2)$. Define
	$$\mathcal L_0(X) = \big\{U\in L^2(\Omega) \ : \ U=\sum_{i=1}^n a_i \big(X(t_i)-m(t_i)\big),\ a_i\in{\mathbb R},\ t_i\in[0,1],\ n\in{\mathbb N}\big\},$$
	where $m(t)={\mathbb E}[X(t)]$, and let $\mathcal L(X)$ be the completion of $\mathcal L_0(X)$ in  $L^2(\Omega)$; hence, in other words, is a closed subspace of $L^2(\Omega)$ defined as the linear span of the centred one-dimensional marginals of the process $X$. It turns out that the transformation $\Psi_X$, from  $\mathcal L(X)$ to $\mathcal H(K)$, defined by
	\begin{equation}\label{eq:isometry}
	\Psi_X(U)(s) = {\mathbb E}[ U(X(s)-m(s))] = \langle U, X(s)-m(s) \rangle \in \mathcal H(K), \ \text{ for } U\in \mathcal L(X)
	\end{equation}
	is an isometry (sometimes called Lo\`eve's isometry) between $\mathcal L(X)$ and $\mathcal H(K)$, that is, $\Psi_X(U)$ is bijective and preserves the inner product. As a consequence, the Hilbert spaces $\mathcal L(X)$ and $\mathcal H(K)$ can be identified.

	In more informal terms, this isometry is nothing but the completion of
	$$
	\sum_ia_iK(t_i,\cdot)\longleftrightarrow \sum_ia_i(X(t_i)-m(t_i))
	$$

\subsection*{Alternative definitions}

	Assume that $K(s,t)$ is continuous. Denote by ${\mathcal K}$ the (linear continuous) covariance operator ${\mathcal K}:L^2[0,1]\rightarrow L^2[0,1]$ defined by

	$$
	{\mathcal K}(x)(t)=\int_0^1K(s,t)x(s)ds
	$$

	Assume that ${\mathcal K}$ is injective.
	We know, from Spectral Theorem, that
	$${\mathcal K}(x)=\sum_i\lambda_i\langle x,e_i\rangle e_i,$$

	$\lambda_i$ and $e_i$ being the eigenvalues and eigenfunctions of ${\mathcal K}$.

		One could also define the Reproducing Kernel Hilbert Space associated with $K$, by
$$
	{\mathcal H}(K)=\left\{x\in L^2[0,1]:  \sum_{i=1}^\infty\frac{\langle x,e_i\rangle^2}{\lambda_i} <\infty \right\}
	$$
	with the inner product and norm given by
	$$
	\langle f,g\rangle_K=\sum_{i=1}^\infty \frac{\langle f,e_i\rangle \langle g,e_i\rangle}{\lambda_i},\ \Vert f\Vert_K^2=\sum_{i=1}^\infty \frac{\langle f,e_i\rangle^2 }{\lambda_i}
	$$
An equivalent definition:
	${\mathcal H}(K)={\mathcal K}^{1/2}(L^2[0,1])$,
	where, from Spectral Theorem, ${\mathcal K}^{1/2}(x)=\sum_i\lambda_i^{1/2}\langle x,e_i\rangle e_i$. It can be easily shown that ${\mathcal K}(L^2) \varsubsetneq {\mathcal K}^{1/2}(L^2)=\mathcal H(K)$ (the inclusion is straightforward, and $K(s, \cdot)\notin \mathcal K(L^2)$).

\subsection*{General properties}

An important thing to note is that in general the trajectories of the process \(X\) \textbf{do not belong to the corresponding RKHS}. More precisely, \(X(t)\notin \mathcal H_k\) with probability \(1\) if \(\mathcal K\) has infinite non-zero eigenvalues. While a rigorous proof of this fact is given in \sidetextcite{hajek1962linear}, an heuristic argument presented in \sidetextcite{wahba1990spline} is as follows. Consider the Karhunen-Loève expansion of \(X\), i.e.,
\[
  X(t) = \sum_{j=1}^\infty \zeta_j \phi_j(t),
\]
and the truncated version \(X_N(t)\) up to the \(N\)-th term. On the one hand, \(X_N(t)\to X(t)\) in the quadratic mean (for each fixed \(t\)) by the Karhunen-Loève theorem, but on the other hand we have
\[
  \mathds E\|X_N(\cdot)\|^2_k = \mathds E \sum_{j=1}^N \frac{\zeta_j^2}{\lambda_j} = N \to \infty \quad (N\to\infty).
\]

Thus, functions in the RKHS are in a way \textit{smoother} than the trajectories of the process themselves. For convenience, if \(x=x(\omega)\) is a realization of \(X\) (for a fixed \(\omega\)), we will denote \(\langle x, f\rangle_k \equiv \left[\Psi_X^{-1}(f)\right](\omega)\). Some interesting properties stemming from this interpretation are the following (see \sidetextcite[][p.~974]{parzen1961approach}):

\begin{enumerate}

  \item \(\langle X, k(\cdot, t)\rangle_k = X(t) - m(t)\).

  \item \(\mathds{E} \langle X, f\rangle_k = \langle m, f\rangle_k\).

  \item \(\cov \left(\langle X, f \rangle_k , \langle X, g\rangle_k\right) = \langle f, g\rangle_k\).

\end{enumerate}

\subsection*{Convergence properties}

Note that
\[
		\sup_t \|\langle f_n,K(\cdot,t)\rangle_K\|=\sup_t|f_n(t)|\leq \Vert f_n\Vert_K \sup_t\Vert K(\cdot,t)\Vert_K
		=\Vert f_n\Vert_K \sup_t K(t,t),
\]
		so $\Vert f_n\Vert_K \to 0$ implies $f_n(t)\to 0$ uniformly on $t$, that is, convergence in the RKHS norm \textbf{entails uniform convergence}.
