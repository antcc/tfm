\documentclass[11pt]{article}
\usepackage[margin=1.3in]{geometry}
%\usepackage[compact,small]{titlesec}
%--------------------------------------------------------
\usepackage{sectsty,todonotes}
\sectionfont{\normalsize\bfseries}
\subsectionfont{\normalsize\sf}
%----------------------------------------------------
\usepackage[mathcal]{euscript}

\usepackage{bm}                     %AMS Packages
\usepackage{amsfonts, framed}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}               %Graphics
%\usepackage{pstricks,pst-plot}
%\usepackage{wrapfig}
\usepackage{natbib}                 %Citation style
\usepackage{colortbl}               %Tables
\usepackage{booktabs}
\usepackage{hyperref}
\setlength{\bibsep}{1pt}
%\usepackage{hypernat}

% Definiciones

\definecolor{violet}{rgb}{0.7,0,0.6}
\newcommand{\explainindetail}[1]{\todo[color=violet]{#1}}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\eps}{\varepsilon}
\newcommand{\ind}{\mathbb{I}}
\newcommand{\pr}{\mathbb{P}}
\newcommand{\B}{B}
\newcommand{\A}{\mathcal{A}}\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\newcommand{\E}{\mathcal{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\F}{\mathfrak{F}}
\newcommand{\G}{\mathbb{G}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\ds}{\displaystyle}
\newcommand{\D}{\mathcal{D}}
\newcommand{\deb}{\stackrel{\mathcal{L}}{\longrightarrow}}
\newcommand{\Vor}{\mbox{Vor}}
\newcommand{\ang}{\measuredangle}
\def\cals_+{{\cals_+}}
\def\cala{{\mathcal{A}}}
\def\calb{{\mathcal{B}}}
\def\calc{{\mathcal{C}}}
\def\cald{{\mathcal{D}}}
\def\calf{{\mathcal{F}}}
\def\calh{{\mathcal{H}}}
\def\call{{\mathcal{L}}}
\def\cals{{\mathcal{S}}}
\def\calv{{\mathcal{V}}}
\def\calg{{\mathcal{G}}}
\def\calp{{\mathcal{P}}}
\def\cale{{\mathcal{E}}}
\def\calo{{\mathcal{O}}}
\def\calj{{\mathcal{J}}}
\def\caln{{\mathcal{N}}}
\def\calm{{\mathcal{M}}}

\hypersetup{
  colorlinks=true,
  citecolor=violet
}

\begin{document}

\begin{center}
\large \bf  Preliminaries on RKHS's\rm
\end{center}

RKHS's are spaces of ``regular'' FUNCTIONS (not equivalence classes in $L^2[0,1]$) with a different norm to that inherited from $L^2[0,1]$.

\section*{Definition}

	Let $K(s,t)$ be a positive definite (the matrix $K(t_i,t_j)$ is always positive definite) real function on $[0,1]^2$.

	Let $\calh_0(K):=\{f\in L^2[0,1] \ : \ f(\cdot)=\sum_{i=1}^n a_i K(t_i,\cdot),\ a_i\in{\mathbb R},\ t_i\in[0,1],\ n\in{\mathbb N}\}$, be
	the space of all finite linear combinations of evaluations of $K$.  This space is endowed with the inner product
	$\langle f,g\rangle_K=\sum_{i,j}\alpha_i\beta_j K(t_i,s_j)$,
	where $f(\cdot)=\sum_i\alpha_i K(t_i,\cdot)$ and $g(\cdot)=\sum_j\beta_j K(s_j,\cdot)$.

	Then, the RKHS associated with $K$ is defined as the completion of $\calh_0(K)$. In other words, $\calh(K)$ is made of all functions obtained as pointwise limits of Cauchy sequences in $\calh_0(K)$. The inner product is extended accordingly to the whole space $\calh(K)$.

\section*{Reproducing property}

	These spaces are named after the so-called \textit{reproducing property},
	$\langle f, K(s,\cdot)\rangle_K = f(s)$, for all  $f\in\calh(K), s\in[0,1]$, which is particularly important in the applications. On account of this property it is sometimes said that the RKHS are spaces of ``true functions'', in the sense that the pointwise values $f(s)$ at a given $s$ do matter, in contrast with $L^2[0,1]$ whose elements are in fact equivalence classes of functions.

  Note that in an RKHS the evaluation functional is continuous, so a small perturbation $f\mapsto f + \delta f$ results in a small change in $f(x)$ for all $x$.


\section*{Loève's isometry}

	A property of RKHS's especially useful in statistical applications is given by the following \textit{Lo\`eve's isometry}:  let $L^2(\Omega) $ be the Hilbert space of real random variables with finite second moment, endowed with the usual inner product whose associated norm is $\| U\|^2={\mathbb E}(U^2)$. Define
	$$\call_0(X) = \big\{U\in L^2(\Omega) \ : \ U=\sum_{i=1}^n a_i \big(X(t_i)-m(t_i)\big),\ a_i\in{\mathbb R},\ t_i\in[0,1],\ n\in{\mathbb N}\big\},$$
	where $m(t)={\mathbb E}[X(t)]$, and let $\call(X)$ be the completion of $\call_0(X)$ in  $L^2(\Omega)$; hence, in other words, is a closed subspace of $L^2(\Omega)$ defined as the linear span of the centred one-dimensional marginals of the process $X$. It turns out that the transformation $\Psi_X$, from  $\call(X)$ to $\calh(K)$, defined by
	\begin{equation}\label{eq:isometry}
	\Psi_X(U)(s) = {\mathbb E}[ U(X(s)-m(s))] = \langle U, X(s)-m(s) \rangle \in \calh(K), \ \text{ for } U\in \call(X)
	\end{equation}
	is an isometry (sometimes called Lo\`eve's isometry) between $\call(X)$ and $\calh(K)$, that is, $\Psi_X(U)$ is bijective and preserves the inner product. As a consequence, the Hilbert spaces $\call(X)$ and $\calh(K)$ can be identified.

	\textbf{In more informal terms, this isometry is nothing but the completion} of
	$$
	\sum_ia_iK(t_i,\cdot)\longleftrightarrow \sum_ia_i(X(t_i)-m(t_i))
	$$

\section*{Alternative definitions}

	Assume that $K(s,t)$ is continuous. Denote by ${\mathcal K}$ the (linear continuous) covariance operator ${\mathcal K}:L^2[0,1]\rightarrow L^2[0,1]$ defined by

	$$
	{\mathcal K}(x)(t)=\int_0^1K(s,t)x(s)ds
	$$

	Assume that ${\mathcal K}$ is injective.
	We know, from Spectral Theorem, that
	$${\mathcal K}(x)=\sum_i\lambda_i\langle x,e_i\rangle e_i,$$

	$\lambda_i$ and $e_i$ being the eigenvalues and eigenfunctions of ${\mathcal K}$.

		One could also define the Reproducing Kernel Hilbert Space associated with $K$, by
$$
	{\mathcal H}(K)=\left\{x:\in L^2[0,1]:  \sum_{i=1}^\infty\frac{\langle x,e_i\rangle^2}{\lambda_i} <\infty \right\}
	$$
	with the inner product and norm given by
	$$
	\langle f,g\rangle_K=\sum_{i=1}^\infty \frac{\langle f,e_i\rangle \langle g,e_i\rangle}{\lambda_i},\ \Vert f\Vert_K^2=\sum_{i=1}^\infty \frac{\langle f,e_i\rangle^2 }{\lambda_i}
	$$
An equivalent definition:
	${\mathcal H}(K)={\mathcal K}^{1/2}(L^2[0,1])$,
	where, from Spectral Theorem, ${\mathcal K}^{1/2}(x)=\sum_i\lambda_i^{1/2}\langle x,e_i\rangle e_i$

\section*{General properties}

  An important thing to note is that in general the trajectories of the process $X$ \textbf{do not belong to the corresponding RKHS}. More precisely, $X(t)\notin \mathcal H(K)$ with probability $1$ if $\mathcal K$ has infinite non-zero eigenvalues. While a rigorous proof of this fact is given in \citet{hajek1962}, an heuristic argument presented in \citet{wahba1990} is as follows. Consider the Karhunen-Loève expansion of $X$, i.e.,
  \[
  X(t) = \sum_{j=1}^\infty \zeta_j \phi_j(t),
  \]

  and the truncated version $X_N(t)$ up to the $N$-th term. On the one hand, $X_N(t)\to X(t)$ in the quadratic mean (for each fixed $t$) by the Karhunen-Loève theorem, but on the other hand we have
  \[
\mathbb E\|X_N(\cdot)\|^2_K = \mathbb E \sum_{j=1}^N \frac{\zeta_j^2}{\lambda_j} = N \to \infty \quad (N\to\infty).
  \]

  Thus, functions in the RKHS are in a way ``smoother'' than the trajectories of the process themselves. For convenience, if $x=x(\omega)$ is a realization of $X$ (for a fixed $\omega$), we will denote $\langle x, f\rangle_K \equiv \left[\Psi_X^{-1}(f)\right](\omega)$. Some interesting properties stemming from this interpretation are the following (see \citet[p.~974]{parzen1961b}):

  \begin{enumerate}

    \item $\langle X, K(t, \cdot)\rangle_K = X(t) - m(t)$

    \item $\mathbb{E} \langle X, h\rangle_K = \langle m, h\rangle_K$

    \item $\operatorname{Cov}\left(\langle X, h \rangle_K , \langle X, g\rangle_K\right) = \langle h, g\rangle_K$

  \end{enumerate}


\section*{Convergence properties}

Note that
\[
		\sup_t \|\langle f_n,K(\cdot,t)\rangle_K\|=\sup_t|f_n(t)|\leq \Vert f_n\Vert_K \sup_t\Vert K(\cdot,t)\Vert_K
		=\Vert f_n\Vert_K \sup_t K(t,t),
\]
		so $\Vert f_n\Vert_K \to 0$ implies $f_n(t)\to 0$ uniformly on $t$, that is, convergence in the RKHS norm \textbf{entails uniform convergence}.

\end{itemize}




\begin{thebibliography}{9}
	\bibitem[\protect\citeauthoryear{Hsing \& Eubank}{2015}]{hsi15}
	Hsing, T., \& Eubank, R. (2015). \textit{Theoretical foundations of functional data analysis, with an introduction to linear operators} (Vol. 997). John Wiley \& Sons.

	\bibitem[\protect\citeauthoryear{Paulsen \& Raghupathi }{2015}]{pau16}
	Paulsen, V. I., \& Raghupathi, M. (2016). \textit{An introduction to the theory of reproducing kernel Hilbert spaces} (Vol. 152). Cambridge University press.
  \bibitem[\protect\citeauthoryear{Parzen}{1961}]{parzen1961b} Parzen, E. (1961). \textit{An Approach to Time Series Analysis}. The Annals of Mathematical Statistics 32(4), 951–989.

  \bibitem[\protect\citeauthoryear{Wahba}{1990}]{wahba1990} Wahba G. (1990). \textit{Spline models for observational data}. CBMS 59, SIAM, Philadelphia.

  \bibitem[\protect\citeauthoryear{Hajek}{1962}]{hajek1962} Hajek J. (1962). \textit{On linear Statistical problems in stochastic processes}. Chekoslovak Math. J., vol. 12, pp.404-443.

\end{thebibliography}

\end{document}
