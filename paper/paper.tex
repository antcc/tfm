% \documentclass[ba,preprint]{imsart}% use this for supplement article
\documentclass[ba]{imsart}
%
\pubyear{2022}
\volume{TBA}
\issue{TBA}
\doi{0000}
\arxiv{2010.00000}
\firstpage{1}
\lastpage{1}

%
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue,filecolor=blue,backref=page]{hyperref}
\usepackage{graphicx}

\startlocaldefs
\numberwithin{equation}{section}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}{Proposition}[section]

\renewcommand{\epsilon}{\varepsilon}

\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\endlocaldefs

%% Environment for colored comments
\newenvironment{comment}
{
\noindent \em \color{red}
}
{
\color{black}
}

%% Inline comments
\newcommand\incomment[1]{\color{red}[\textit{#1}]\color{black}}

\begin{document}

\begin{frontmatter}
\title{Bayesian RKHS-based methods in functional regression\support{Support information of the article.}}
\runtitle{Bayesian RKHS-based methods in functional regression }

\begin{aug}
\author{\fnms{First} \snm{Author}\thanksref{addr1,t1,t2,m1}\ead[label=e1]{first@somewhere.com}},
\author{\fnms{Second} \snm{Author}\thanksref{addr1,t3,m1,m2}\ead[label=e2]{second@somewhere.com}}
\and
\author{\fnms{Third} \snm{Author}\thanksref{addr2,t1,m2}%
\ead[label=e3]{third@somewhere.com}%
\ead[label=u1,url]{http://www.foo.com}}

\runauthor{F. Author et al.}

\address[addr1]{Address of the First and Second authors
     Usually a few lines long
    \printead{e1} % print email address of "e1"
    \printead*{e2}
}

\address[addr2]{Address of the Third author
    Usually a few lines long
    Usually a few lines long
    \printead{e3}
    \printead{u1}
}

\thankstext{t1}{Some comment}
\thankstext{t2}{First supporter of the project}
\thankstext{t3}{Second supporter of the project}

\end{aug}

\begin{abstract}
In this work we propose a Bayesian approach for functional linear or logistic  regression models, based on the theory of Reproducing Kernel Hilbert Spaces (RKHS's). These new models build upon the RKHS associated with the covariance function of the underlying stochastic process, and can be viewed as a finite-dimensional alternative to the classical functional regression paradigm. The corresponding functional model (or the functional logistic equation in the case of binary response) is determined by a function living on a dense subset of the RKHS generated by the underlying covariance function. By imposing a suitable prior distribution on such RKHS, we can perform data-driven inference via standard Bayes methodology. The posterior distribution can be approximated from Markov Chain Monte Carlo (MCMC) methods. Several estimators  derived from this posterior distribution turn out to be competitive against other usual  alternatives in both simulated examples and real datasets.
\end{abstract}

\begin{keyword}[class=MSC]
\kwd[Primary ]{62M20}
\kwd[; secondary ]{62F15}
\end{keyword}

\begin{keyword}
\kwd{functional data}
\kwd{linear regression}
\kwd{logistic regression}
\kwd{reproducing kernel Hilbert space}
\kwd{Bayesian inference}
\end{keyword}

\end{frontmatter}

\section{Introduction}

\subsection{A brief review of the theory of RKHS's}

The methodology proposed in this work relies heavily on the use of RKHS's, so before diving into it we will briefly describe the main characteristics of these spaces. In what follows, suppose \(X=X(t)\) is an \(L^2\)-process with trajectories in \(L^2[0,1]\), and further assume that its mean function \(m(t)=\mathbb E[X(t)]\) and the covariance function \(K(s, t)= \mathbb E[(X(t) - m(t)(X(s) - m(s)))]\) are both continuous. To construct the RKHS associated with the covariance function, we start by defining the functional space \(H_0(K)\) of all finite linear combinations of evaluations of \(K\), that is,
\[
H_0(K) = \left\{ f \in L^2[0,1]: f(\cdot) = \sum_{i=1}^p a_i K(\cdot, t_i), \ p \in \N, \ a_i \in \R, \ t_i \in [0, 1] \right\}.
\]
This space is endowed with the inner product
\[
\langle f, g\rangle_K = \sum_{i, j} a_i b_j K(s_i, t_j),
\]
given that \(f(\cdot)=\sum_i a_i K(\cdot, s_i) \) and \(g(\cdot)=\sum_j b_j K(\cdot, t_j)\). Lastly, we define \(H(K)\) to be the completion of \(H_0(K)\) under the norm induced by the scalar product defined above.

As we can see, the covariance function plays a crucial role in characterizing the RKHS. An operator closely related to this covariance function is the so-called covariance operator, i.e.:
\[
\mathcal Kf(\cdot) = \int_0^1 K(\cdot, t)f(t)\, dt.
\]
This operator is especially relevant due to the \textit{Karhunen-Loève} theorem \incomment{cita}, which asserts that \(X\) can be decomposed as
\[
X(t) = \sum_{j=1}^\infty \xi_j \phi_j(t),
\]
where \(\{\phi_j\}\) are the eigenfunctions of the covariance operator \(\mathcal K\) and \(\xi_j = \int X\phi_j\) are uncorrelated zero-mean random variables with variance equal to \(\lambda_j\), the eigenvalue associated with \(\phi_j\):



There are at least three alternative, equivalent ways of defining the RKHS associated with \(K\), and each of them gives a different insight into the relationship between the process itself and the underlying RKHS.

\begin{enumerate}
  \item Loeve's isometry
  \item Sum with lambda j
  \item Square root of K

\end{enumerate}3

\subsection{Overview of Markov chain Monte Carlo methods}

\section{Bayesian methodology for RKHS-based functional regression models}

Note that starting from a probability distribution \(\mathbb{P}_0\) on \(H_0(K)\) we can obtain a probability distribution \(\mathbb{P}\) on \(H(K)\) simply by defining \(\mathbb{P}(B) = \mathbb{P}_0(B\cap H_0(K))\) for all Borel sets \(B\). The following result proves that any such extension is indeed unique.

\begin{prop} Let \(H\) be a separable Hilbert space and consider a dense subset \(H_0\subseteq H\). Then, any probability distribution on \((H, \mathcal{B}(H))\) is uniquely determined by its restriction to \((H_0, \mathcal B(H_0))\).
\end{prop}

\begin{comment}
  ¿Es necesario especificar que \(\mathcal B\) es la sigma álgebra de Borel?
\end{comment}

\begin{comment}
    ¿La prueba va en el cuerpo del artículo, o en un anexo/material suplementario?
\end{comment}

\begin{proof}

First, note that Theorem 7.1.2 in \citet[p.~177]{hsing2015theoretical} establishes that the distribution of any random variable \(X: (\Omega, \mathcal A, \mathbb{P})\to (H, \mathcal B(H))\) is uniquely determined by the distribution of its random projections \(\langle X, h\rangle_H\) for all \(h \in H\).
Consider two random variables \(\alpha\) and \(\beta\), taking values in \(H\) and equally distributed on \(H_0\), and let \(h \in H\). Since \(H_0\) is dense in \(H\), there is a sequence \(\{h_n\}\subseteq H_0\) such that \(\|h_n - h\|_H \to 0\), and necessarily \(\langle \alpha, h_n\rangle_H \equiv \langle \beta, h_n\rangle_H\) for all \(n\), where the equality is in distribution.

Observe now that \(\langle \alpha, h_n\rangle_H \overset{P}{\to} \langle \alpha, h\rangle_H\), since
\[
\forall \epsilon > 0 \quad \mathbb{P}\{|\langle \alpha, h_n - h\rangle_H| > \epsilon\} \leq \mathbb{P}\{\|\alpha\|_H \|h_n - h\|_H > \epsilon\} \to 0,
\]
and equivalently \(\langle \beta, h_n \rangle_H \overset{P}{\to} \langle \beta, h\rangle_H\), so that \(\langle \alpha, h \rangle_H \equiv \langle \beta,h\rangle_H\). As this is valid for every \(h \in H\), we can conclude that \(\alpha \equiv \beta\) on \(H\).
\end{proof}


\subsection{Functional linear regression}

\subsection{Functional logistic regression}

\section{Experimental results}

\subsection{Simulation studies}

\subsection{Application to real data}

\section{Conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Supplementary Material, if any, should   %%
%% be provided in {supplement} environment  %%
%% with title and short description.        %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{supplement}
\stitle{Title of Supplement A}
\sdescription{Short description of Supplement A.}
\end{supplement}

\bibliographystyle{ba}
\bibliography{sample}

\begin{acks}[Acknowledgments]
This is an acknowledgements section.
\end{acks}


\end{document}
