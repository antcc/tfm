% \documentclass[ba,preprint]{imsart}% use this for supplement article
\documentclass[ba]{imsart}
%
\pubyear{2022}
\volume{TBA}
\issue{TBA}
\doi{0000}
\arxiv{2010.00000}
\firstpage{1}
\lastpage{1}

%
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue,filecolor=blue,backref=page]{hyperref}
\usepackage{graphicx}

\startlocaldefs
\numberwithin{equation}{section}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}{Proposition}[section]

\renewcommand{\epsilon}{\varepsilon}

\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}

%% Scalar product
\newcommand\dotprod[2]{\left\langle #1,\, #2 \right\rangle}

%% Environment for colored comments
\newenvironment{comment}
{
\noindent \em \color{red}
}
{
\color{black}
}

%% Inline comments
\newcommand\incomment[1]{\color{red}[\textit{#1}]\color{black}}
\endlocaldefs

\begin{document}

\begin{frontmatter}
\title{Bayesian RKHS-based methods in functional regression\support{Support information of the article.}}
\runtitle{Bayesian RKHS-based methods in functional regression }

\begin{aug}
\author{\fnms{First} \snm{Author}\thanksref{addr1,t1,t2,m1}\ead[label=e1]{first@somewhere.com}},
\author{\fnms{Second} \snm{Author}\thanksref{addr1,t3,m1,m2}\ead[label=e2]{second@somewhere.com}}
\and
\author{\fnms{Third} \snm{Author}\thanksref{addr2,t1,m2}%
\ead[label=e3]{third@somewhere.com}%
\ead[label=u1,url]{http://www.foo.com}}

\runauthor{F. Author et al.}

\address[addr1]{Address of the First and Second authors
     Usually a few lines long
    \printead{e1} % print email address of "e1"
    \printead*{e2}
}

\address[addr2]{Address of the Third author
    Usually a few lines long
    Usually a few lines long
    \printead{e3}
    \printead{u1}
}

\thankstext{t1}{Some comment}
\thankstext{t2}{First supporter of the project}
\thankstext{t3}{Second supporter of the project}

\end{aug}

\begin{abstract}
In this work we propose a Bayesian approach for functional linear or logistic  regression models, based on the theory of Reproducing Kernel Hilbert Spaces (RKHS's). These new models build upon the RKHS associated with the covariance function of the underlying stochastic process, and can be viewed as a finite-dimensional alternative to the classical functional regression paradigm. The corresponding functional model (or the functional logistic equation in the case of binary response) is determined by a function living on a dense subset of the RKHS generated by the underlying covariance function. By imposing a suitable prior distribution on such RKHS, we can perform data-driven inference via standard Bayes methodology. The posterior distribution can be approximated from Markov Chain Monte Carlo (MCMC) methods. Several estimators  derived from this posterior distribution turn out to be competitive against other usual  alternatives in both simulated examples and real datasets.
\end{abstract}

\begin{keyword}[class=MSC]
\kwd[Primary ]{62M20}
\kwd[; secondary ]{62F15}
\end{keyword}

\begin{keyword}
\kwd{functional data}
\kwd{linear regression}
\kwd{logistic regression}
\kwd{reproducing kernel Hilbert space}
\kwd{Bayesian inference}
\end{keyword}

\end{frontmatter}

\begin{comment}
  Algunos comentarios generales:
\end{comment}

\section{Introduction}\label{sec:intro}

The problem of inferring a scalar response \(Y\) from a functional covariate \(X=X(t)\) is one that has gained traction over the last few decades, as more and more data is being generated with an ever-increasing level of granularity in the measurements. While in principle we could simply treat the functional data as a discretized vector in a very high dimension, there are often many advantages in taking into account the functional nature of the data, ranging from modeling the possibly high correlation among points that are close in the domain, to extracting information that may be hidden in the derivatives of the function in question. As a consequence, numerous proposals have arisen on how to suitably treat functional data, all of them encompassed under the term Functional Data Analysis (FDA), which essentially explores statistical techniques to process, model and make inference on continuous data. A recent survey on such techniques and methods is \citet{cuevas2014partial}, while a more detailed exposition of the theory and applications can be found for example in \citet{hsing2015theoretical} or \citet{berrendero2019rkhs}.

In this work we are concerned with (supervised) functional linear and logistic regression models, that is, situations where the goal is to predict a continuous or dichotomous variable from functional observations. Even though these problems can be formally stated with almost no differences from their finite-dimensional counterparts, there are some fundamental challenges that emerge as a result of working in infinite dimensions. To set a common framework, throughout this work we will consider a scalar response variable \(Y\) (either continuous or binary) which has some hidden dependence on a stochastic \(L^2\)-process \(X=X(t)=X(t, \omega)\) with trajectories in \(L^2[0, 1]\). We will assume the existence of a ``labeled'' data set \(\mathcal D_n =\{(X_i(t), Y_i): i=1,\dots, n\}\) of independent observations from \((X, Y)\), and our aim will be to accurately predict the response corresponding to unlabeled samples from \(X\).

The most common linear regression model is the classical functional linear regression model, originally introduced in the first edition of the book by~\citet{ramsay2005functional} \incomment{fueron realmente los primeros?}. It can be seen as a generalization of the usual finite-dimensional model, replacing the scalar product in \(\R^n\) for that of the functional space \(L^2[0,1]\):
\begin{equation}\label{eq:l2-linear-model}
Y = \dotprod{X}{\beta} + \epsilon = \int_0^1 X(t)\beta(t)\, dt + \epsilon,
\end{equation}
where \(\epsilon\) is an independent error term with certain simplifying assumptions. In this case, the main parameter \(\beta=\beta(t)\) is a member of the infinte-dimensional space \(L^2[0, 1]\). Aside from the fact that functional (i.e. infinite-dimensional) optimization is a hard problem in and of itself \incomment{cita?}, there are a number of more subtle drawbacks that arise in this \(L^2\)-framework. For instance, \(L^2[0, 1]\) is a big space that contains many non-smooth or ill-behaved functions, but incidentally the model given by~\eqref{eq:l2-linear-model} does not include a finite-dimensional model based on a linear combination of projections of \(X\) as a particular case (see \citet{berrendero2019rkhs}). Moreover, the non-invertibility of the covariance operator (defined in Section~\ref{sec:rkhs}), which plays the role of the covariance matrix in the infinite case, invalidates the usual least-squares theory.

A similar \(L^2\)-model can be derived for the binary classification problem via the logistic function:
\begin{equation}\label{l2-logistic-model}
  \mathbb P(Y \mid X(t)) = \frac{1}{}
\end{equation}

It turns out that a natural alternative to the \(L^2\) model is the RKHS model..... Not only simpler, but also smoother and more manageable functions. Sustituir producto escalar por el de \(H(K)\). This model has already been proposed in \incomment{cita Berrendero et al. 2019}, and in \incomment{cita Torrecilla 2017} for the case of functional classification.


Bayesian RKHS-based variable selection (cite Berrendero et als.).

\subsubsection{Organization of the paper}

In Section~\ref{sec:intro} we review...

\subsection{A brief review of the theory of RKHS's}\label{sec:rkhs}

\begin{comment}
  Es posible que esta sección sea demasiado larga...?
\end{comment}

The methodology proposed in this work relies heavily on the use of RKHS's, so before diving into it we will briefly describe the main characteristics of these spaces (for a more detailed account, see for example~\citet{berlinet2004reproducing}). In what follows, suppose \(X=X(t)\) is an \(L^2\)-process with trajectories in \(L^2[0,1]\), and further assume that its mean function \(m(t)=\mathbb E[X(t)]\) and its covariance function \(K(t, s)= \mathbb E[(X(t) - m(t))(X(s) - m(s))]\) are both continuous. To construct the RKHS associated with the covariance function, we start by defining the functional space \(\mathcal H_0(K)\) of all finite linear combinations of evaluations of \(K\), that is,
\[
\mathcal H_0(K) = \left\{ f \in L^2[0,1]: f(\cdot) = \sum_{i=1}^p a_i K(t_i, \cdot), \ p \in \N, \ a_i \in \R, \ t_i \in [0, 1] \right\}.
\]
This space is endowed with the inner product
\[
\dotprod{f}{g}_K = \sum_{i, j} a_i b_j K(t_i, s_j),
\]
given that \(f(\cdot)=\sum_i a_i K(t_i, \cdot) \) and \(g(\cdot)=\sum_j b_j K(s_j, \cdot)\). Then, we define \(\mathcal H(K)\) to be the completion of \(\mathcal H_0(K)\) under the norm induced by the scalar product defined above. As it turns out, functions in this space verify the \textit{reproducing property}, thus giving it its name:
\begin{equation}\label{eq:reproducing-property}
  \dotprod{K(t, \cdot)}{f}_K = f(t), \quad \text{for all } f \in \mathcal H(K), \ t \in [0, 1].
\end{equation}

As we can see, the covariance function (sometimes referred to as the \textit{kernel}) plays a crucial role in characterizing the RKHS \incomment{¿Es necesario destacar alguna suposición/propiedad extra sobre \(K\)? Por ejemplo \(\int\int K^2 <\infty\) o que es simétrica y (semi-) definida positiva}. A compact operator closely related to this covariance function is the so-called covariance operator, i.e.: \incomment{No sé muy bien qué calificativos ponerle: integral Hilbert-Schmidt operator, compact, self-adjoint, bounded, linear, ...}
\[
\mathcal Kf(\cdot) = \int_0^1 K(s, \cdot)f(s)\, ds, \quad f \in L^2[0, 1].
\]
This operator is especially relevant due to the \textit{Karhunen-Loève} theorem \incomment{cita}, which asserts that a centered process \(X\) can be decomposed as
\begin{equation}\label{eq:karhunen-loeve}
X(t) = \sum_{j=1}^\infty \xi_j \phi_j(t),
\end{equation}
where \(\phi_j\) are the (orthonormal) eigenfunctions of its covariance operator \(\mathcal K\) and \(\xi_j = \int X\phi_j\) are uncorrelated zero-mean random variables with variance equal to \(\lambda_j\), the eigenvalue associated with \(\phi_j\). This expansion is the main justification of many FDA techniques, such as functional principal component analysis (FPCA) (see for example \incomment{cita}).

There are at least three alternative, equivalent ways of defining the RKHS associated with \(K\), and each of them gives a different insight into the relationship between the process itself and the underlying RKHS.

\begin{enumerate}
  \item Via \textit{Loeve's isometry}, one can establish a congruence between \(\mathcal H(K)\) and the linear span of the centered process, \(\mathcal L(X)\), in the space of all random variables with finite second moment, \(L^2(\Omega)\) \incomment{cita}. This isometry is essentially the completion of the correspondence
  \[
  \sum_{i=1}^p a_i (X(t_i) - m(t_i)) \longleftrightarrow \sum_{i=1}^p a_i K(t_i, \cdot),
\]
and can be formally defined as (see \incomment{cita})
\begin{equation}\label{eq:loeves-isometry}
  \Psi_X(U)(t) = \E[U(X(t) - m(t))], \quad U \in \mathcal L(X).
\end{equation}
  \item The space \(\mathcal H(K)\) can also be defined as the image of the square root of the covariance operator, i.e.:
  \begin{equation}\label{eq:rkhs-square-root}
  \mathcal H(K) = \mathcal K^{1/2}(L^2[0, 1]).
\end{equation}
In this case, the inner product can be expressed as
\[
\dotprod{f}{g}_K = \dotprod{\mathcal K^{1/2}(f)}{\mathcal K^{1/2}(g)}.
\]

  \item Lastly, starting from the above definition and combining expression~\eqref{eq:karhunen-loeve} and the representation of \(K\) given by \textit{Mercer's theorem} \incomment{cita}, we can also express
  \begin{equation}\label{rkhs-sum-lambda}
    \mathcal H(K) = \left\{f \in L^2[0, 1]: \sum_{j=1}^\infty \frac{\dotprod{f}{\phi_j}^2}{\lambda_j} < \infty \right\},
  \end{equation}
  with the corresponding inner product
  \[
  \dotprod{f}{g}_K = \sum_{j=1}^\infty \frac{\dotprod{f}
  {\phi_j}\dotprod{g}{\phi_j}}{\lambda_j}.
  \]
Note that, since the spectral theorem for compact operators tells us that \(\lambda_j \to 0\) \incomment{cita}, this definition highlights the fact that functions in \(\mathcal H(K)\) are smooth, in the sense that their components in an orthonormal basis need to vanish quickly.
\end{enumerate}

\begin{comment}
  Puede que las definiciones 2 y 3 arriba sean innecesarias.
\end{comment}

Despite the close connection between the process \(X\) and the space \(\mathcal H(K)\), special care must be taken when dealing with concrete realizations of the process, since in general the trajectories do not belong to the RKHS with probability one (see \incomment{cita}). As a consequence, the expression \(\dotprod{x}{f}_K\) is ill-defined when \(x\) is a realization of \(X\). However, following Parzen's approach \incomment{cita}\ we can leverage Loève's isometry and identify \(\dotprod{x}{f}_K \) with \( \Psi_X^{-1}(f)(\omega)\), for \(x=X(\omega)\) and \(f\in \mathcal H(K)\). This notation often proves to be useful and convenient.

\section{Bayesian methodology for RKHS-based functional regression models}

In this section...

We will assume? the notation from the previous section and suppose WLOG that \(X\) is centered.

\begin{comment}

  - Parametric approach with \(\beta \in H_0(K)\) (further constrain the parameter to this subset of the RKHS, which is dense by construction). Then we can impose a prior on the finite-dimensional parameter space.
\end{comment}

The general idea explored in this work revolves around imposing a prior distribution on the functional space \(\mathcal H_0(K)\), which we know is dense in \(\mathcal H(K)\). Specifically, we consider a functional parameter of the form
\begin{equation}\label{eq:parameter-rkhs}
  \beta(t) = \sum_{i=1}^p \beta_i K(t_i, \cdot),
\end{equation}
and regard \(p \in \N\), \(\beta_i \in \R\) and \(t_i \in [0, 1]\) as free parameters in the parameter space \(\Theta = \N \times \R^p \times [0, 1]^p\).

As a matter of fact, to simplify things further we will consider \(\pi(p) = \delta_{\hat p}\), or equivalently, we will consider a fixed value \(\hat p\) for the dimension of the model.

Note that starting from a probability distribution \(\mathbb{P}_0\) on \(\mathcal H_0(K)\) we can obtain a probability distribution \(\mathbb{P}\) on \(\mathcal H(K)\) simply by defining \(\mathbb{P}(B) = \mathbb{P}_0(B\cap \mathcal H_0(K))\) for all Borel sets \(B \in \mathcal B(\mathcal H(K))\). The following result proves that any such extension is indeed unique.

\begin{prop} Let \(H\) be a separable Hilbert space and consider a dense subset \(H_0\subseteq H\). Then, any probability distribution on \((H, \mathcal{B}(H))\) is uniquely determined by its restriction to \((H_0, \mathcal B(H_0))\).
\end{prop}
\begin{comment}
    ¿La prueba va en el cuerpo del artículo, o en un anexo/material suplementario?
\end{comment}

\begin{proof}

First, note that Theorem 7.1.2 in \citet[p.~177]{hsing2015theoretical} establishes that the distribution of any random variable \(X: (\Omega, \mathcal A, \mathbb{P})\to (H, \mathcal B(H))\) is uniquely determined by the distribution of its random projections \(\dotprod{X}{h}_H\) for all \(h \in H\).
Consider two random variables \(\alpha\) and \(\beta\), taking values in \(H\) and equally distributed on \(H_0\), and let \(h \in H\). Since \(H_0\) is dense in \(H\), there is a sequence \(\{h_n\}\subseteq H_0\) such that \(\|h_n - h\|_H \to 0\), and necessarily \(\dotprod{\alpha}{h_n}_H \equiv \dotprod{\beta}{h_n}_H\) for all \(n\), where the equality is in distribution.

Observe now that \(\dotprod{\alpha}{h_n}_H \overset{P}{\to} \dotprod{\alpha}{h}_H\), since
\[
\forall \epsilon > 0 \quad \mathbb{P}\{|\dotprod{\alpha}{h_n - h}_H| > \epsilon\} \leq \mathbb{P}\{\|\alpha\|_H \|h_n - h\|_H > \epsilon\} \to 0,
\]
and equivalently \(\dotprod{\beta}{h_n}_H \overset{P}{\to} \dotprod{\beta}{h}_H\), so that \(\dotprod{\alpha}{h}_H \equiv \dotprod{\beta}{h}_H\). As this is valid for every \(h \in H\), we can conclude that \(\alpha \equiv \beta\) on \(H\).
\end{proof}

\subsubsection{Overview of MCMC methods}

\subsection{Functional linear regression}
 The model remains linear because of the correspondence between the RKHS and the linear span of the process (Loève’s isometry). The RKHS model includes both L2 model and finite model. Cite Berrendero et al 2020.

\begin{comment}
  - Estimators derived.
\end{comment}

\subsection{Functional logistic regression}

In this case the model/functional equation in $L^2$ is....... and the same problems apply, plus the MLE existence problem. An RKHS aproach is given by:.......

 and this solves the problems (see Bueno-Larraz 2021)

\begin{comment}
  - Estimators derived.
\end{comment}

\subsection{Model choice and validation}

\begin{comment}
  - Choice of p, MLE, BIC, ...

  - Posterior predictive checks, ¿Bayesian p-values?, ...
\end{comment}



\section{Experimental results}

\begin{comment}
  Not only MSE/Acc but also credible intervals, points-of-impact model --> variable selection included.
\end{comment}

\subsection{Simulation studies}

\subsection{Application to real data}

\section{Conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Supplementary Material, if any, should   %%
%% be provided in {supplement} environment  %%
%% with title and short description.        %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{supplement}
\stitle{Title of Supplement A}
\sdescription{Short description of Supplement A, with DOI and/or links to additional material (code, ...)}
\end{supplement}

\bibliographystyle{ba}
\bibliography{bibliography}

\begin{acks}[Acknowledgments]
This is an acknowledgements section.
\end{acks}


\end{document}
