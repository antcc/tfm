%%%
% Class 'kaobook' from https://github.com/fmarotta/kaobook
%
% Copyright (c) 2022 Antonio Coín Castro
%%%

\documentclass[
  a4paper,
	fontsize=11pt, % Base font size
	twoside=false, % Use different layouts for even and odd pages (in particular, if twoside=true, the margin column will be always on the outside)
	%open=any, % If twoside=true, uncomment this to force new chapters to start on any page, not only on right (odd) pages
	%chapterprefix=true, % Uncomment to use the word "Chapter" before chapter numbers everywhere they appear
	%chapterentrydots=true, % Uncomment to output dots from the chapter name to the page number in the table of contents
  secnumdepth=2,
	numbers=noenddot, % Comment to output dots after chapter numbers; the most common values for this option are: enddot, noenddot and auto (see the KOMAScript documentation for an in-depth explanation)
	%draft=true, % If uncommented, rulers will be added in the header and footer
	%overfullrule=true, % If uncommented, overly long lines will be marked by a black box; useful for correcting spacing problems
]{kaohandt}

% Choose the language
\ifxetexorluatex
	\usepackage{polyglossia}
	\setmainlanguage{english}
\else
	\usepackage[english]{babel} % Load characters and hyphenation
\fi
\usepackage[english=british]{csquotes} % English quotes

% Change default fonts
\setmathfont[range={\mathcal,\mathbfcal, \mathbb}]{NewCMMath-Regular.otf}

% Change page style
\pagestyle{scrheadings}

% Load the bibliography package
\usepackage[
  style=authoryear-comp
]{kaobiblio}

% Bibliography side cite style
\renewcommand{\formatmargincitation}[1]{%
  \parencite{#1}: \citetitle{#1}.%
}

% Bibliography file
\addbibresource{bibliography.bib}

% Load the package for hyperreferences
\usepackage{kaorefs}

% Set reference colors
\hypersetup{
  pdfauthor = {Antonio Coín},
  pdftitle = {Notes},
  citecolor=teal,
  urlcolor=blue!50!black,
  %anchorcolor=OliveGreen,
  linkcolor=teal
}

% Load mathematical packages for theorems and related environments
\usepackage{kaotheorems}

% Set the paths where to look for images
\usepackage{subcaption}
\graphicspath{{figures/}}

% Only show section titles in margintoc
\setcounter{margintocdepth}{\sectiontocdepth}

% Load additional packages
\usepackage{dsfont}  %\mathds

% Some custom commands
\newcommand{\R} {\ensuremath{\mathds{R}}}
\newcommand{\E} {\ensuremath{\mathds{E}}}
\newcommand{\eps}{\varepsilon}
\newcommand{\bm}{\symbf}

%% Scalar product
\newcommand\dotprod[2]{\left\langle #1,\, #2 \right\rangle}

\DeclareMathOperator{\var} {Var}
\DeclareMathOperator{\cov} {Cov}

%%%%%%%%%%%%%%%
% DOCUMENT
%%%%%%%%%%%%%%%


\begin{document}

\title{Notes}
\author[FM]{Antonio Coín Castro}
\date{\today}
\maketitle

\margintoc[*-7]

\section{Functional Data Analysis}

See \sidetextcite{hsing2015theoretical} for a complete discussion on the subject.

\section{Bayesian Linear Regression}

\marginnote{We assume the functional regressor \(X=X(t)\) to be an \(L^2\)-stochastic process with trajectories in \(L^2[0, 1]\), a vanishing mean function and a continuous covariance function \(K=K(t, s)\).}

We consider the RKHS-based functional linear regression model given by\sidenote[][*7]{This formulation comes from considering the (centered) model \(Y=\Psi_X^{-1}(\alpha) + \eps\), where \(\eps\) is an independent normal error term with \(\E\eps=0\) and \(\var(\eps)=\sigma^2\).}
\begin{equation}
  \label{eq:model_lin}
  Y_i \mid \theta, X_i \stackrel{\text{i.i.d.}}{\sim} \mathcal N(\Psi_{X_i}^{-1}(\alpha), \sigma^2),
\end{equation}
where \(\Psi_X\) is \textit{Loeve's isometry} and \(\alpha \in \mathcal H(K)\) is the regression coefficient function. The main idea is to restrict \(\alpha\) to the linear span of a finite subset of evaluations of the kernel \(\{K(\cdot, t_j)\}_j\), since the whole set of such evaluations is dense in \(\mathcal H(K)\). Specifically, we only consider functions of the form
\begin{equation}
  \label{eq:alpha}
  \alpha(t) = \sum_{j=1}^p \beta_j K(t, t_j),
\end{equation}
where \(p\) is a fixed hyperparameter. Under these assumptions, the parametric space is the set \(\Theta \subseteq \R^p \times [0, 1]^p \times \R^+\), with elements
\[
  \theta = (\beta_1, \dots, \beta_p, t_1,\dots, t_p, \sigma^2)' \equiv (\beta, \tau, \sigma^2)'.
\]

Following a Bayesian approach, we can set a prior distribution on the parameters, such as:
\begin{align*}
  f(\sigma^2)              & \propto 1/\sigma^2,                                                     \\
  \tau                     & \sim \mathscr U([0, 1]^p),                                              \\
  \beta\mid \tau, \sigma^2 & \sim \mathcal N(b_0, g\sigma^2(\mathcal X_\tau' \mathcal X_\tau)^{-1}),
\end{align*}
where\sidenote[][*1]{This is because of the correspondence between \(K(\cdot, t_j)\) and \(X(t_j)\), which becomes relevant when plugging the approximation~\eqref{eq:alpha} into the model~\eqref{eq:model_lin}.}
\(\mathcal X_\tau(i, j)=X_i(t_j)\). The prior on \(\beta\) is known as Zellner's \(g\)-prior owing to the main hyperparameter \(g\in \R^+\), which controls the confidence we have in the initial estimate \(b_0 \in \R^p\) for the true value of the coefficients. The choice of a non-informative improper prior for \(\sigma^2\) (known as Jeffreys prior) is of course subject to change. All in all, the prior distribution for the parameters becomes
\begin{equation}
  \label{eq:prior}
  \pi(\theta) = \pi(\beta, \tau, \sigma^2) = \pi(\beta \mid \tau, \sigma^2)\pi(\tau \mid \sigma^2)\pi(\sigma^2).
\end{equation}

For the inference step we rely on Bayes' theorem, which in the case of i.i.d. samples amounts to
\[
  \pi(\theta \mid Y_1, \dots, Y_n) \propto \left[ \prod_{i=1}^n \pi(Y_i\mid \theta) \right]\pi(\theta),
\]
that is, the \textit{posterior} is proportional to the \textit{likelihood} times the \textit{prior}. For convenience, let \(G_\tau=\mathcal X_\tau' \mathcal X_\tau\) and \(Y = (Y_1,\dots, Y_n)'\). Then, substituting the expressions derived for our model above, we get:
\begin{widepar}
  \begin{align*}
    \pi(\theta \mid Y) & \propto \left[ \prod_{i=1}^n  \frac{1}{\sqrt{2\pi}\sigma} \exp\left\{ -\frac{1}{2\sigma^2} \left(Y_i - \sum_{j=1}^{p}\beta_j X_i(t_j)\right)^2 \right\}\right]\frac{|G_\tau|^{1/2}}{(2\pi g)^{p/2}\sigma^{p+2}} \exp\left\{ -\frac{1}{2g\sigma^2} (\beta - b_0)'G_\tau(\beta - b_0)\right\} \\
                       & \propto \frac{|G_\tau|^{1/2}}{\sigma^{p+n+2}} \exp\left\{ -\frac{1}{2\sigma^2} \left(\|Y- \mathcal X_\tau\beta\|^2 + \frac{1}{g}(\beta - b_0)'G_\tau(\beta - b_0) \right) \right\}.
  \end{align*}
\end{widepar}
Note that the resulting expression is not a true distribution, since we have omitted the possibly intractable integral related to the normalizing constant. Nevertheless, we can sample from the posterior distribution\sidenote[][*0]{Provided that the \textit{marginal likelihood} \(\int_\Theta \pi(Y\mid \theta)\pi(\theta)\, d\theta\) is finite.} using several Markov chain Monte Carlo (MCMC) sampling algorithms, which are discussed in \refsubsec{subsec:mcmc}.

Finally, to recover the marginal posterior distribution of each parameter, we ``only'' need to integrate out the desired quantities, e.g.:
\[
  \pi(\beta\mid Y) = \int_{\R^+}\int_{\R^p} \pi(\beta, \tau, \sigma^2\mid Y)\, d\tau\, d\sigma^2.
\]
However, since these integrals can be hard to compute, in most cases we can also exploit the MCMC sampling algorithms to approximate the marginal distributions directly, or even derive other Bayesian estimators such as the posterior expected value or a MAP\marginnote[*-1]{MAP stands for \textit{Maximum a Posteriori Probability}.} estimate for each parameter.
\subsection{MCMC methods}
\labsubsec{subsec:mcmc}

In most MCMC methods it is preferable to work with the \textit{log-posterior} probability, both in terms of efficiency and stability. In our case, we have:
\begin{widepar}
  \[
    \log \pi(\theta\mid Y) \propto \frac{1}{2}\log |G_\tau| - (p+n+2)\log \sigma -\frac{1}{2\sigma^2} \left(\|Y- \mathcal X_\tau\beta\|^2 + \frac{1}{g}(\beta - b_0)'G_\tau(\beta - b_0) \right).
  \]
\end{widepar}

\subsubsection{Gibbs sampling}

Some notes:

\begin{enumerate}
  \item To compute initial values: random, or sample from prior.
  \item It is more efficient to work with the natural logarithm of the objective function and then exponentiate when needed (to avoid overflow and underflow).
  \item Gibbs sampling is better than standard Metropolis due to the curse of dimensionality and the concentration of density in small regions.
  \item We can obtain the marginal posterior directly, considering only samples for each parameter. We can also compute other estimates such as expectation, or a MAP estimate, by binning samples and choosing the ones that occur the most.
  \item \textbf{Always} allow for a burn-in initial period (how many samples?).
  \item To compute expectation: average only every \(n\)-th value (because of correlation).
\end{enumerate}

Some possible modifications of the methods include:

\begin{enumerate}
  \item Use \textit{collapsed Gibbs} or \textit{blocked Gibbs} to sample from more than one variable at a time. For this it would be useful to derive a graphical Bayesian model for the parameters.
  \item Use \textit{simulated annealing} to try to reduce the autocorrelation of samples.

\end{enumerate}

\subsubsection{Affine-invariant ensemble sampler}

See \sidetextcite[*0]{foreman2013emcee}.

\subsubsection{Probabilistic programming in Python: PyMC3}

See \sidetextcite[*0]{salvatier2016probabilistic}.

\subsection{Model extensions}

Some immediate extensions to the model are the following:
\begin{enumerate}

  \item Include \(p\) in the set of model parameters and define a prior distribution on it as well.

  \item Consider \textit{multiple} functional covariates \(\alpha_1(t), \dots, \alpha_m(t)\) and adapt the model (e.g. via Gibbs sampling) to this new setting, considering \(\theta=(\beta^{(1)}, \tau^{(1)}, \dots, \beta^{(m)}, \tau^{(m)}, \sigma^2)\).

  \item Change the prior distribution of the parameters in various ways: impose an informative prior on \(\sigma^2\), change the prior on \(\tau\) to the Dirichlet distribution on the hypercube, or even introduce prior information on the kernel \(k\) when defining the prior on \(\beta\).

  \item Substitute the matrix \(\mathcal X\) for \(\mathcal X + \eta I\) as a form of regularization, to avoid ill-conditioning problems and to increase the stability of the computations.

  \item Derive Bayesian estimates of the support and/or the coefficient function by minimizing a loss function integrated with respect to the posterior distribution, as done in \sidetextcite[*-2]{grollemund2019bayesian}. In particular, it is worth exploring the \textit{prior and posterior predictive probabilities} of the parameters to gain insight into them (as explained \href{https://en.wikipedia.org/wiki/Bayesian_inference#Bayesian_prediction}{here} and illustrated \href{https://docs.pymc.io/en/stable/pymc-examples/examples/diagnostics_and_criticism/posterior_predictive.html}{here}).

\end{enumerate}

\section{Bayesian Logistic Regression}

-- Work in progress --

\section{Notes on the theory of RKHS's}

  \textbf{Why choose the parameter in the RKHS?}

  - Small RKHS norm implies small variations and hence a higher degree of smoothness. Specifically, \(\|f\|_K\) controls how fast the function varies over the input space with respect to the geometry defined by the kernel:
  \[
  |f(t) - f(s)| = |\dotprod{f}{K_t - K_s}_K| \leq \|f\|_K \|K_t - K_s\|_K.
  \]

  - Note that in an RKHS the evaluation functional is continuous, so a small perturbation \(f\mapsto f + \delta f\) results in a small change in \(f(x)\) for all \(x\). Not equivalence classes.

  - The model is \textit{linear} because of the correspondence between the RKHS and the linear span of the process (Loève's isometry).

\input{notes-rkhs.tex}

%%%%%%%%%%%%%%%
% BIBLIOGRAPHY
%%%%%%%%%%%%%%%

\defbibnote{bibnote}{Here are the references in citation order.\par\bigskip} % Prepend this text to the bibliography
\renewcommand*{\bibfont}{\small}
\printbibliography[title=Bibliography]%, prenote=bibnote]


%%%%%%%%%%%%%%%
% APPENDICES
%%%%%%%%%%%%%%%

\appendix

\section{Further study}

Here are some ideas and additional materials.

\begin{enumerate}

  \item \textbf{Asymptotic properties of standard Bayesian functional linear regression.} In the paper by Lian et al. (2016) they consider a Gaussian process prior on the slope coefficient and show some asymptotic properties of the posterior distribution (which is also Gaussian).

  \item \textbf{Posterior concentration for a misspecified Bayesian regression model with functional covariates.} Theoretical paper by Abraham and Grollemund (2020).

  \item \textbf{Bliss method in R.} \url{https://rdrr.io/cran/bliss/f/vignettes/BlissIntro.Rmd}

\end{enumerate}
\end{document}
