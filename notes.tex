%%%
% Class 'kaobook' from https://github.com/fmarotta/kaobook
%
% Copyright (c) 2022 Antonio Coín Castro
%%%

\documentclass[
  a4paper,
	fontsize=11pt, % Base font size
	twoside=false, % Use different layouts for even and odd pages (in particular, if twoside=true, the margin column will be always on the outside)
	%open=any, % If twoside=true, uncomment this to force new chapters to start on any page, not only on right (odd) pages
	%chapterprefix=true, % Uncomment to use the word "Chapter" before chapter numbers everywhere they appear
	%chapterentrydots=true, % Uncomment to output dots from the chapter name to the page number in the table of contents
  secnumdepth=2,
	numbers=noenddot, % Comment to output dots after chapter numbers; the most common values for this option are: enddot, noenddot and auto (see the KOMAScript documentation for an in-depth explanation)
	%draft=true, % If uncommented, rulers will be added in the header and footer
	%overfullrule=true, % If uncommented, overly long lines will be marked by a black box; useful for correcting spacing problems
]{kaohandt}

% Choose the language
\ifxetexorluatex
	\usepackage{polyglossia}
	\setmainlanguage{english}
\else
	\usepackage[english]{babel} % Load characters and hyphenation
\fi
\usepackage[english=british]{csquotes} % English quotes

% Change default fonts
\setmathfont[range={\mathcal,\mathbfcal, \mathbb}]{NewCMMath-Regular.otf}

% Change page style
\pagestyle{scrheadings}

% Load the bibliography package
\usepackage[
  style=authoryear-comp
]{kaobiblio}

% Bibliography side cite style
\renewcommand{\formatmargincitation}[1]{%
  \parencite{#1}: \citetitle{#1}.%
}

% Bibliography file
\addbibresource{bibliography.bib}

% Load the package for hyperreferences
\usepackage{kaorefs}

% Set reference colors
\hypersetup{
  pdfauthor = {Antonio Coín},
  pdftitle = {Notes},
  citecolor=teal,
  urlcolor=blue!50!black,
  %anchorcolor=OliveGreen,
  linkcolor=teal
}

% Load mathematical packages for theorems and related environments
\usepackage{kaotheorems}

% Set the paths where to look for images
\usepackage{subcaption}
\graphicspath{{figures/}}

% Only show section titles in margintoc
\setcounter{margintocdepth}{\sectiontocdepth}

% Load additional packages
\usepackage{dsfont}  %\mathds

% Some custom commands
\newcommand{\R} {\ensuremath{\mathds{R}}}
\newcommand{\E} {\ensuremath{\mathds{E}}}
\newcommand{\eps}{\varepsilon}
\newcommand{\bm}{\symbf}
\DeclareMathOperator{\var} {Var}
\DeclareMathOperator{\cov} {Cov}

%%%%%%%%%%%%%%%
% DOCUMENT
%%%%%%%%%%%%%%%


\begin{document}

\title{Notes}
\author[FM]{Antonio Coín Castro}
\date{\today}
\maketitle

\margintoc[*-2]

\section{Questions}

Here are some questions.

\begin{enumerate}

  \item Why does it not make sense to regress \(Y\) on a finite combination of projections of \(X\) whenever we consider the \(L^2\) model?

  \item Why is it that \(k(\cdot, t) \notin \mathcal K(L^2)\)?

  \item We are assuming that \(\beta, \tau\) and \(\sigma^2\) are independent? Independent given \(Y\) and/or \(X\)?

  \item How many samples is it usually enough to successfully predict a relatively simple model? What about the execution time?

  \item How to set a sensible value for \(g\) and \(b_0\)? Cross-validation?

  \item Which MCMC sampling algorithm should we use?

  \item Can we ignore the posterior on \(\sigma^2\) and treat it as a nuisance parameter?

  \item \textbf{How to include the case in which} \(\bm{\mathds EX(t)\neq 0}\) \textbf{and/or} \(\bm{\mathds EY\neq 0}\)? Do we add a term \(\mu\) in the model and impose a prior on it as well? How would we treat the terms \(X_i(t_j) - m(t_j)\) resulting from \(\Psi^{-1}_{X_i}(k(\cdot, t_j))\)? Can we just subtract the sample mean to every sample trajectory and response and work with our current model?

  \item How do we get an estimate of the parameters given the posterior distribution?

  \item How do we try the algorithm in a more realistic setting? Which is the standard error measure for the latent parameters?

\end{enumerate}

\newpage

\section{Bayesian Linear Regression}

\marginnote{We assume the functional regressor \(X=X(t)\) to be an \(L^2\)-stochastic process with trajectories in \(L^2[0, 1]\), a vanishing mean function and a continuous covariance function \(k=k(s, t)\).}

We consider the RKHS-based functional linear regression model given by\sidenote[][*7]{This formulation comes from considering the (centered) model \(Y=\Psi_X^{-1}(\alpha) + \eps\), where \(\eps\) is an independent normal error term with \(\E\eps=0\) and \(\var(\eps)=\sigma^2\).}
\begin{equation}
  \label{eq:model_lin}
  Y_i \mid \theta, X_i \stackrel{\text{i.i.d.}}{\sim} \mathcal N(\Psi_{X_i}^{-1}(\alpha), \sigma^2),
\end{equation}
where \(\Psi_X\) is \textit{Loeve's isometry} and \(\alpha \in \mathcal H_{k}\) is the regression coefficient function. The main idea is to restrict \(\alpha\) to the linear span of a finite subset of evaluations of the kernel \(\{k(\cdot, t_j)\}_j\), since the whole set of such evaluations is dense in \(\mathcal H_k\). Specifically, we only consider functions of the form
\begin{equation}
  \label{eq:alpha}
  \alpha(t) = \sum_{j=1}^p \beta_j k(t, t_j),
\end{equation}
where \(p\) is a fixed hyperparameter. Under these assumptions, the parametric space is the set \(\Theta \subseteq \R^p \times [0, 1]^p \times \R^+\), with elements
\[
  \theta = (\beta_1, \dots, \beta_p, t_1,\dots, t_p, \sigma^2)' \equiv (\beta, \tau, \sigma^2)'.
\]

Following a Bayesian approach, we can set a prior distribution on the parameters, such as:
\begin{align*}
  f(\sigma^2)              & \propto 1/\sigma^2,                                                     \\
  \tau                     & \sim \mathscr U([0, 1]^p),                                              \\
  \beta\mid \tau, \sigma^2 & \sim \mathcal N(b_0, g\sigma^2(\mathcal X_\tau' \mathcal X_\tau)^{-1}),
\end{align*}
where\sidenote[][*1]{This is because of the correspondence between \(k(\cdot, t_j)\) and \(X(t_j)\), which becomes relevant when plugging the approximation~\eqref{eq:alpha} into the model~\eqref{eq:model_lin}.}
\(\mathcal X_\tau(i, j)=X_i(t_j)\). The prior on \(\beta\) is known as Zellner's \(g\)-prior owing to the main hyperparameter \(g\in \R^+\), which controls the confidence we have in the initial estimate \(b_0 \in \R^p\) for the true value of the coefficients. The choice of a non-informative improper prior for \(\sigma^2\) is of course subject to change. All in all, the prior distribution for the parameters becomes
\begin{equation}
  \label{eq:prior}
  \pi(\theta) = \pi(\beta, \tau, \sigma^2) = \pi(\beta \mid \tau, \sigma^2)\pi(\tau \mid \sigma^2)\pi(\sigma^2).
\end{equation}

For the inference step we rely on Bayes' theorem, which in the case of i.i.d. samples amounts to
\[
  \pi(\theta \mid Y_1, \dots, Y_n) \propto \left[ \prod_{i=1}^n \pi(Y_i\mid \theta) \right]\pi(\theta),
\]
that is, the \textit{posterior} is proportional to the \textit{likelihood} times the \textit{prior}. For convenience, let \(G_\tau=\mathcal X_\tau' \mathcal X_\tau\) and \(Y = (Y_1,\dots, Y_n)'\). Then, substituting the expressions derived for our model above, we get:
\begin{widepar}
  \begin{align*}
    \pi(\theta \mid Y) & \propto \left[ \prod_{i=1}^n  \frac{1}{\sqrt{2\pi}\sigma} \exp\left\{ -\frac{1}{2\sigma^2} \left(Y_i - \sum_{j=1}^{p}\beta_j X_i(t_j)\right)^2 \right\}\right]\frac{|G_\tau|^{1/2}}{(2\pi g)^{p/2}\sigma^{p+2}} \exp\left\{ -\frac{1}{2g\sigma^2} (\beta - b_0)'G_\tau(\beta - b_0)\right\} \\
                       & \propto \frac{|G_\tau|^{1/2}}{\sigma^{p+n+2}} \exp\left\{ -\frac{1}{2\sigma^2} \left(\|Y- \mathcal X_\tau\beta\|^2 + \frac{1}{g}(\beta - b_0)'G_\tau(\beta - b_0) \right) \right\}.
  \end{align*}
\end{widepar}
Note that the resulting expression is not a true distribution, since we have omitted the possibly intractable integral related to the normalizing constant. Nevertheless, we can sample from the posterior distribution\sidenote[][*0]{Provided that the \textit{marginal likelihood} \(\int_\Theta \pi(Y\mid \theta)\pi(\theta)\, d\theta\) is finite.} using several Markov chain Monte Carlo (MCMC) sampling algorithms, which are discussed in \refsubsec{subsec:mcmc}.

Finally, to recover the marginal posterior distribution of each parameter, we ``only'' need to integrate out the desired quantities, e.g.:
\[
  \pi(\beta\mid Y) = \int_{\R^+}\int_{\R^p} \pi(\beta, \tau, \sigma^2\mid Y)\, d\tau\, d\sigma^2.
\]
However, since these integrals can be hard to compute, in most cases we can also exploit the MCMC sampling algorithms to approximate the marginal distributions directly, or even derive other Bayesian estimators such as the posterior expected value or a MAP\marginnote[*-1]{MAP stands for \textit{Maximum a Posteriori Probability}.} estimate for each parameter.
\subsection{MCMC methods}
\labsubsec{subsec:mcmc}

In most MCMC methods it is preferable to work with the \textit{log-posterior} probability, both in terms of efficiency and stability. In our case, we have:
\begin{widepar}
  \[
    \log \pi(\theta\mid Y) \propto \frac{1}{2}\log |G_\tau| - (p+n+2)\log \sigma -\frac{1}{2\sigma^2} \left(\|Y- \mathcal X_\tau\beta\|^2 + \frac{1}{g}(\beta - b_0)'G_\tau(\beta - b_0) \right).
  \]
\end{widepar}

\subsubsection{Gibbs sampling}

Some notes:

\begin{enumerate}
  \item To compute initial values: random, or sample from prior.
  \item It is more efficient to work with the natural logarithm of the objective function and then exponentiate when needed (to avoid overflow and underflow).
  \item Gibbs sampling is better than standard Metropolis due to the curse of dimensionality and the concentration of density in small regions.
  \item We can obtain the marginal posterior directly, considering only samples for each parameter. We can also compute other estimates such as expectation, or a MAP estimate, by binning samples and choosing the ones that occur the most.
  \item \textbf{Always} allow for a burn-in initial period (how many samples?).
  \item To compute expectation: average only every \(n\)-th value (because of correlation).
\end{enumerate}

Some possible modifications of the methods include:

\begin{enumerate}
  \item Use \textit{collapsed Gibbs} or \textit{blocked Gibbs} to sample from more than one variable at a time. For this it would be useful to derive a graphical Bayesian model for the parameters.
  \item Use \textit{simulated annealing} to try to reduce the autocorrelation of samples.

\end{enumerate}

\subsubsection{Affine-invariant ensemble sampler}

See \sidetextcite[*0]{foreman2013emcee}.

\subsubsection{Resources for MCMC methods}

Some resources on MCMC methods in Python are:

\begin{enumerate}
  \item \href{https://www.tweag.io/blog/2019-10-25-mcmc-intro1/}{A step-by-step tutorial}
  \item \href{http://www.stat.columbia.edu/~gelman/research/published/nuts.pdf}{NUTS sampler}
  \item \href{https://github.com/michaelnowotny/pyjags}{pyjags}
  \item \href{https://www.cdslab.org/paramonte/index.html}{ParaMonte}
  \item \href{https://github.com/prmiles/pymcmcstat}{pymcmcstat}
  \item \href{https://docs.pymc.io/}{\textbf{PyMC3}}
  \item \href{https://mc-stan.org/}{Stan}
  \item \href{https://emcee.readthedocs.io/en/stable/tutorials/quickstart/#quickstart}{\textbf{emcee}}
\end{enumerate}

\subsection{Model extensions}

Some immediate extensions to the model are the following:
\begin{enumerate}

  \item Include \(p\) in the set of model parameters and define a prior distribution on it as well.

  \item Consider \textit{multiple} functional covariates \(\alpha_1(t), \dots, \alpha_m(t)\) and adapt the model (e.g. via Gibbs sampling) to this new setting, considering \(\theta=(\beta^{(1)}, \tau^{(1)}, \dots, \beta^{(m)}, \tau^{(m)}, \sigma^2)\).

  \item Change the prior distribution of the parameters in various ways: impose an informative prior on \(\sigma^2\), change the prior on \(\tau\) to the Dirichlet distribution on the hypercube, or even introduce prior information on the kernel \(k\) when defining the prior on \(\beta\).

  \item Substitute the matrix \(\mathcal X\) for \(\mathcal X + \eta I\) as a form of regularization, to avoid ill-conditioning problems and to increase the stability of the computations.

  \item Derive Bayesian estimates of the support and/or the coefficient function by minimizing a loss function integrated with respect to the posterior distribution, as done in \sidetextcite[*-2]{grollemund2019bayesian}. In particular, it is worth exploring the \textit{prior and posterior predictive probabilities} of the parameters to gain insight into them (as explained \href{https://en.wikipedia.org/wiki/Bayesian_inference#Bayesian_prediction}{here}).

\end{enumerate}

\section{Bayesian Logistic Regression}

-- Work in progress --

%%%%%%%%%%%%%%%
% BIBLIOGRAPHY
%%%%%%%%%%%%%%%

\defbibnote{bibnote}{Here are the references in citation order.\par\bigskip} % Prepend this text to the bibliography
\renewcommand*{\bibfont}{\small}
\printbibliography[title=Bibliography]%, prenote=bibnote]


%%%%%%%%%%%%%%%
% APPENDICES
%%%%%%%%%%%%%%%

\appendix

\section{Notes on the theory of RKHS's}

An important thing to note is that in general the trajectories of the process \(X\) \textbf{do not belong to the corresponding RKHS}. More precisely, \(X(t)\notin \mathcal H_k\) with probability \(1\) if \(\mathcal K\) has infinite non-zero eigenvalues. While a rigorous proof of this fact is given in \sidetextcite{hajek1962linear}, an heuristic argument presented in \sidetextcite{wahba1990spline} is as follows. Consider the Karhunen-Loève expansion of \(X\), i.e.,
\[
  X(t) = \sum_{j=1}^\infty \zeta_j \phi_j(t),
\]
and the truncated version \(X_N(t)\) up to the \(N\)-th term. On the one hand, \(X_N(t)\to X(t)\) in the quadratic mean (for each fixed \(t\)) by the Karhunen-Loève theorem, but on the other hand we have
\[
  \mathds E\|X_N(\cdot)\|^2_k = \mathds E \sum_{j=1}^N \frac{\zeta_j^2}{\lambda_j} = N \to \infty \quad (N\to\infty).
\]

Thus, functions in the RKHS are in a way \textit{smoother} than the trajectories of the process themselves. For convenience, if \(x=x(\omega)\) is a realization of \(X\) (for a fixed \(\omega\)), we will denote \(\langle x, f\rangle_k \equiv \left[\Psi_X^{-1}(f)\right](\omega)\). Some interesting properties stemming from this interpretation are the following (see \sidetextcite[][p.~974]{parzen1961approach}):

\begin{enumerate}

  \item \(\langle X, k(\cdot, t)\rangle_k = X(t) - m(t)\).

  \item \(\mathds{E} \langle X, f\rangle_k = \langle m, f\rangle_k\).

  \item \(\cov \left(\langle X, f \rangle_k , \langle X, g\rangle_k\right) = \langle f, g\rangle_k\).

\end{enumerate}

%\include{notes-rkhs.tex}

\section{Further study}

Here are some ideas and additional materials.

\begin{enumerate}

  \item \textbf{Asymptotic properties of standard Bayesian functional linear regression.} In the paper by Lian et al. (2016) they consider a Gaussian process prior on the slope coefficient and show some asymptotic properties of the posterior distribution (which is also Gaussian).

  \item \textbf{Posterior concentration for a misspecified Bayesian regression model with functional covariates.} Theoretical paper by Abraham and Grollemund (2020).

  \item \textbf{Bliss method in R.} \url{https://rdrr.io/cran/bliss/f/vignettes/BlissIntro.Rmd}

\end{enumerate}
\end{document}
