%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Copyright (c) 2022 Antonio Co√≠n
%
% This work is licensed under a
% Creative Commons Attribution-ShareAlike 4.0 International License.
%
% You should have received a copy of the license along with this
% work. If not, see <http://creativecommons.org/licenses/by-sa/4.0/>.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusions}\label{ch:conclusions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this work we have addressed some inference and prediction issues involved in the task of learning from functional data. In particular, we have tackled functional linear and logistic regression problems, presenting an alternative to the \(L^2\)-models predominant in these scenarios. Our proposed models rely heavily on the theory of RKHS's, offering finite-dimensional approximations to the regression function, but still with a functional perspective. This leads to a conceptually simpler approach that facilitates the interpretation and subsequent analysis, which we believe is a compelling feature for most practitioners. The underlying theoretical aspects are nevertheless far from trivial, so we have also explored some of the related mathematical background in this text.

With the aid of Bayesian methods, we have introduced a computationally feasible approach to parameter estimation within the proposed RKHS models. The flexibility is improved through the use of prior distributions that can introduce pre-existing information into the model. In addition, the fact that we have a complete posterior distribution at our disposal enables us to derive different prediction and variable selection methods, or even perform other types of inference such as the construction of credible intervals. Moreover, the use of MCMC methods to approximate the posterior distribution results in an implementation that is simpler than most existing alternatives. Although this simplicity comes at the expense of increased computational requirements, this computational cost is still reasonable and the model can be used in practice. The execution time only starts to be of some relevance when performing extensive simulations and comparisons, but it is not a serious burden for the use of our methods in practical cases.

We implemented the whole inference and prediction pipeline in the Python programming language, and then used it to carry out a large empirical study to assess the performance of our models against other usual alternatives, functional and otherwise. The results were generally satisfactory, proving that we have competitive methods that achieve good performance in different scenarios, often with low dimensionality and thus increased tractability and interpretability. Lastly, we believe that with more computational resources we could consider a larger pool of hyperparameters in the model selection phase, and also increase the number of folds and random train/test splits, which would most likely contribute to better results.

\section{Future work}

Some lines of further research suggested by our work are the following:

\begin{itemize}
  \item To delve further into the relationship between RKHS's and functional data problems, a connection that has proven to be fruitful in many scenarios. A first idea would be to extend the RKHS-based logistic regression model to a generalized functional linear model with an arbitrary link function.
  \item To try to derive some theoretical properties of our Bayesian predictors. For example, consistency and/or robustness results regarding the posterior distribution would be an excellent complement to the practical side of this work.
  \item To find other prior distributions for our parameters that perform better in general, or to eliminate the need of the hyperparameters \(b_0\) or \(\eta\).
  \item To experiment with other MCMC algorithms for posterior approximation. Specifically, it would be interesting to implement an efficient and reliable reversible-jump MCMC method in Python, which as we have already mentioned is a better fit for our particular Bayesian model. Moreover, we could also adopt a different approach and use variational inference methods to approximate the posterior.
\end{itemize}
