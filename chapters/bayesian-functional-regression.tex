%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Copyright (c) 2022 Antonio Coín
%
% This work is licensed under a
% Creative Commons Attribution-ShareAlike 4.0 International License.
%
% You should have received a copy of the license along with this
% work. If not, see <http://creativecommons.org/licenses/by-sa/4.0/>.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\let\epsilon\varepsilon

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Bayesian methodology for RKHS-based functional regression models}\label{ch:bayesian}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this chapter we present the precise functional models and Bayesian methodologies explored in this work. The RKHS-based functional models under consideration \citep[see][]{berrendero2018functional, berrendero2019rkhs} are those obtained by substituting the functional parameter \(\beta\in L^2[0,1]\) for \(\alpha \in \Hcal(K)\), replacing also the scalar product \(\dotprod{X}{\beta}\) for \(\dotprod{X}{\alpha}_K\) in the \(L^2\)-models~\eqref{eq:l2-linear-model} and~\eqref{eq:l2-logistic-model}. However, to further simplify things we will follow a parametric approach and suppose that \(\alpha\) is in fact a member of the dense subspace \(\Hcal_0(K)\) defined in~\eqref{eq:h0}, i.e.:
\begin{equation}\label{eq:alpha_h0}
\alpha(\cdot) = \sum_{j=1}^p \beta_j K(t_j, \cdot), \text{ for some } p \in \N, \ \beta_j \in \R \text{ and } t_j \in [0,1].
\end{equation}
Moreover, as we said before, with a slight abuse of notation we will understand the expression \(\dotprod{x}{\alpha}_K\) as \(\Psi_x(\alpha)\), where \(x=X(\omega)\) and \(\Psi_x\) is Loève's isometry. Hence, taking into account that \(\Psi_X(K(t, \cdot)) = X(t)\) by definition (see~\eqref{eq:loeves-isometry}), we can write \(\dotprod{x}{\alpha}_K \equiv \sum_{j=1}^p \beta_j x(t_j)\) when \(\alpha\) is as in~\eqref{eq:alpha_h0}.

In this way we get a simpler, finite-dimensional approximation of the functional RKHS model, which we argue reduces the overall complexity of the model while still capturing most of the relevant information. When it comes to parameter estimation, a direct optimization of some loss function would probably require a tailored algorithm that took into account the continuous nature of the times \(t_j\). Indeed, such an idea is explored in \citet{berrendero2018functional} for the logistic regression case, where the authors propose a ``greedy max-max'' method reminiscent of the EM algorithm that alternates between estimating the coefficients and the time instants through a maximum likelihood approach.

At this point we propose to follow a Bayesian approach to estimate the parameters of the model, which we believe is in line with the idea of simplicity we pursue, and also introduces an additional layer of flexibility in the model. In this way, we can include problem-specific information in the model through the use of prior distributions, and on top of that, this method works almost unaltered for both linear and logistic regression models. The general idea will be to impose a prior distribution on the functional parameter to eventually derive a posterior model after incorporating the available sample information. In view of~\eqref{eq:alpha_h0}, to set a prior distribution on the unknown function \(\alpha\) (that is, a prior distribution on the functional space \(\Hcal_0(K)\)) it suffices to first consider a discrete distribution on \(p\), and then impose \(p\)-dimensional continuous prior distributions on the coefficients \(\beta_j\) and the times \(t_j\) given \(p\). Thanks to this parametric approach, the challenging task of setting a prior distribution on a space of functions is considerably simplified, while simultaneously not constraining the model to any specific distribution (in contrast to, for instance, Gaussian process regression methods). Moreover, note that starting from a probability distribution \(\P_0\) on \(\Hcal_0(K)\) we can obtain a probability distribution \(\P\) on \(\Hcal(K)\) merely by defining \(\P(B) = \P_0(B\cap \Hcal_0(K))\) for all Borel sets \(B\). Consequently, our simplifying assumption on \(\alpha\) is not very restrictive, since any prior distribution on \(\Hcal_0(K)\) can be directly extended to a prior distribution on \(\Hcal(K)\).

However, after some intial experimentation we found that, for practical and computational reasons, the value of \(p\), the dimensionality of the model, is best fixed beforehand in a suitable way (see Chapter~\ref{ch:model-choice} for details). Thus, we will regard only the \(\beta_j\) and \(t_j\) as free parameters, and search for our functional parameter in the space
\begin{equation}\label{eq:h0p}
\Hcal_{0,p}(K)=\left\{ \sum_{j=1}^p \beta_j K(t_j, \cdot): \ \beta_j \in \R, \ t_j \in [0, 1]\right\}.
\end{equation}
Even though we actually work on \(\Hcal_{0,p}(K)\), the discrete parameter \(p\) can still be selected in several meaningful ways that make use of the available data, and the set of feasible values is not very large in practice. Moreover, we could think of this approach as imposing a degenerate prior distribution on \(p\), so it is in a way a particular case of the more general model discussed above.

In any case, after selecting a suitable prior distribution \(\pi(\theta)\) for the finite-dimensional parameter vector \(\theta\) (which will be specified shortly), we can resort to Bayes' theorem to perform the inference step, which in the case of i.i.d. samples amounts to
\begin{equation}\label{eq:bayes-theorem}
  \pi(\theta \mid \D_n) \propto \left( \prod_{i=1}^n \pi(Y_i\mid X_i, \theta) \right)\pi(\theta).
\end{equation}
In Sections~\ref{sec:rkhs-linear-model} and~\ref{sec:rkhs-logistic-model} we proceed to specify the parameter spaces, prior distributions and concrete models for \(\pi(Y | X,\theta)\) considered in the case of functional linear regression and functional logistic regression, respectively. Even though in~\eqref{eq:bayes-theorem} we have omitted the possibly intractable integral related to the normalizing constant, sampling from the (approximate) posterior distribution can still be accomplished via MCMC methods (see Chapter~\ref{ch:model-choice} for implementation details).

\section{Functional linear regression}\label{sec:rkhs-linear-model}

In the case of functional linear regression, the simplified RKHS model considered in this work is given by
\begin{equation}\label{eq:rkhs-model-linear}
  Y = \alpha_0 + \Psi_X(\alpha) + \epsilon = \alpha_0 + \sum_{j=1}^p \beta_j X(t_j) + \epsilon,
\end{equation}
where \(\alpha(\cdot) = \sum_{j=1}^p \beta_j K(t_j, \cdot) \in \Hcal_{0,p}(K)\), \(\alpha_0\in\R\), and \(\epsilon \sim \mathcal N(0, \sigma^2)\) is an error term independent from \(X\). This model is essentially a finite-dimensional approximation from a functional perspective to the more general RKHS model that assumes \(\alpha \in \Hcal(K)\), proposed in~\citet{berrendero2019rkhs}.

When \(p\) is fixed, the parameter space becomes \(\Theta_p = \R^p \times [0, 1]^p \times \R \times \R^+\), and in the sequel a generic element of this \((2p+2)\)-dimensional space will be denoted by \(\theta = (\beta_1,\dots, \beta_p, t_1,\dots, t_p, \alpha_0, \sigma^2) \equiv (b, \tau, \alpha_0, \sigma^2)\). Before proceeding any further, observe that we can rewrite model~\eqref{eq:rkhs-model-linear} in a more explicit and practical fashion in terms of the available sample information in \(\D_n\). For \(\theta \in \Theta_p\), the reinterpreted model assumes the form
\begin{equation}\label{eq:rkhs-model-linear-2}
  Y_i \mid X_i, \theta \ \stackrel{\text{i.i.d.}}{\sim} \mathcal N\left(\alpha_0 + \sum_{j=1}^p \beta_j X_i(t_j), \ \sigma^2\right), \quad i =1,\dots, n.
\end{equation}

It is worth mentioning that the model remains linear in the sense that it fundamentally involves random variables belonging to the linear span of the process \(X\) in \(L^2(\Omega)\). Also, note that given the time instants \(t_j\), the model becomes a multiple linear model with the \(X(t_j)\) as scalar covariates. As a matter of fact, this RKHS model is particularly suited as a basis for variable selection methods, and also entails the classical \(L^2\)-model~\eqref{eq:l2-linear-model} under certain conditions \citep[see][Sec.~3]{berrendero2020general}. In addition, this model could be easily extended to the case of several covariates via an expression of type \(Y=\alpha_0 + \Psi_{X^{1}}(\alpha_1) + \cdots + \Psi_{X^{q}}(\alpha_q) + \epsilon\). In that case, as argued in \citet{grollemund2019bayesian} for a similar situation, if we were to set a prior distribution on all the parameters involved, we could recover the full posterior by looking alternately at the posterior distribution of each covariate conditional on the rest of them.

\subsection*{The Bayesian approach: prior and posterior}

The prior distribution suggested for the parameter vector \(\theta \in \Theta_p\) is given by
\begin{align}\label{eq:prior-linear}
  \begin{split}
  \pi(\alpha_0, \sigma^2)              & \propto 1/\sigma^2,                                                     \\
  \tau                     & \sim \U([0, 1]^p),                                              \\
  b\mid \tau, \sigma^2 & \sim \mathcal N_p(b_0, g\sigma^2{\underbrace{\left(\X_\tau' \X_\tau + \eta I\right)}_{G_\tau}}^{-1}),
\end{split}
\end{align}
where \(I\) is the identity matrix, \(\X_\tau\) is the data matrix \((X_i(t_j))_{i,j}\), and \(b_0\in \R^p, \ g \in \R\) and \(\eta \in \R^+\) are hyperparameters of the model. On the one hand, note the use of a joint prior distribution on \(\alpha_0\) and \(\sigma^2\), which is a widely used non-informative prior known in the standard linear regression setting as Jeffrey's prior \citep{jeffreys1946invariant}. In any event, the estimation of \(\alpha_0=\E[Y]\) is straightforward, so it could have been left out of the model altogether. On the other hand, the prior on \(b\) is a slight modification of the well-known Zellner's g-prior \citep{zellner1986assessing}, in which a regularizing term is added to avoid ill-conditioning problems in the Gram matrix, obtaining a ridge-like Zellner prior controlled by the tuning parameter \(\eta\) \citep{baragatti2012study}. All in all, with a slight abuse of notation the proposed prior distribution becomes \(\pi(\theta) = \pi(b| \tau, \sigma^2)\pi(\tau)\pi(\alpha_0, \sigma^2)\).

As for the posterior distribution, we only compute a function proportional to its log-density, since that is all that is needed for a MCMC algorithm to work. A standard algebraic manipulation in~\eqref{eq:bayes-theorem} yields the following result.

\begin{proposition}
Under the linear model~\eqref{eq:rkhs-model-linear-2}, the prior distribution implied in~\eqref{eq:prior-linear} produces the log-posterior distribution
\begin{align*}
\log \pi(\theta \mid \D_n) \propto {} & \frac{1}{2\sigma^2}\left(\|\symbf Y- \alpha_0\symbf{1} - \X_\tau b\|^2 + \frac{1}{g}(b - b_0)'G_\tau(b - b_0) \right)\\
& + (p+n+2)\log\sigma - \frac{1}{2}\log |G_\tau|,
\end{align*}
where \(\symbf Y=(Y_1,\dots,Y_n)'\) and \(\symbf{1}\) is an \(n\)-dimensional vector of ones.
\end{proposition}

\subsection*{Making predictions}

In order to generate predictions, let us recall that when performing the empirical posterior approximation, on each of the \(M\) steps of the iterative MCMC algorithm we get an approximate sample \(\theta^{(m)*}=(b^{(m)*}, \tau^{(m)*}, \alpha_0^{(m)*}, (\sigma^2)^{(m)*})\) of the posterior distribution \(\pi(\theta| \D_n)\). Assuming now a previously unseen test set \(\D'_{n'}\) in the same conditions as \(\D_n\), we propose to construct three different kinds of predictors based on the MCMC samples, each of them following a different strategy.

  \paragraph{Summarize-then-predict.} If we consider a point-estimate statistic \(T\) that acts as a summary of the marginal posterior distributions, we can get the corresponding estimates \(\hat{\theta}=(\hat b, \hat \tau, \hat{\alpha}_0, \hat{\sigma}^2) = T\{\theta^{(m)*}\} \equiv (T\{b^{(m)*}\}, T\{\tau^{(m)*}\}, T\{\alpha_0^{(m)*}\}, T\{(\sigma^2)^{(m)*}\})\), and then predict the responses in the usual way following model~\eqref{eq:rkhs-model-linear}, i.e.:
  \begin{equation}\label{eq:summarize-predict-linear}
    \hat Y_i = \hat{\alpha}_0 + \sum_{j=1}^p \hat{\beta}_j X_i(\hat{t}_j), \quad i=1,\dots, n'.
  \end{equation}
  Note that in this case the variance \(\sigma^2\) is treated as a nuisance parameter. Although it contributes to measure the uncertainty in the approximations, its estimates are discarded in the final prediction.

  \paragraph{Predict-then-summarize.} Alternatively, we can  look at the approximate posterior distribution as a whole and compute the predictive distribution of the simulated responses at each step of the chain following model~\eqref{eq:rkhs-model-linear-2}:
  \begin{equation}\label{eq:sampled-response-vector}
  \symbf Y^{(m)*} := \left\{Y_i^{(m)*} \equiv Y_i \mid X_i, \theta^{(m)*}:\ i=1,\dots,n'\right\}, \quad m=1,\dots,M.
  \end{equation}
  Then, we can take the mean of all such simulated responses as a proxy for each response variable, that is,
  \[
  \hat Y_i = \frac{1}{M}\sum_{m=1}^M Y_i^{(m)*}, \quad i=1,\dots,n'.
  \]
  This method differs from the previous one in that it takes into account the full approximate posterior distribution instead of summarizing it directly.

  \paragraph{Variable selection.} Lastly, we can focus only on the marginal posterior distribution of \(\tau|\D_n\) and select \(p\) time instants using a point-estimate statistic \(T\) as in our first strategy, but discarding the rest of the parameters. Specifically, we can consider the times \(\hat t_j = T\{t_j^{(m)*}\}\) and reduce the original data set to just the \(n\times p\) real matrix given by \(\{X_i(\hat t_j): i=1, \dots,n, \ j=1,\dots,p\}\). After this variable selection has been carried out, we can tackle the problem using a finite-dimensional linear regression model and apply any of the well-known prediction algorithms suited for this situation.\\

Note that these predictors can be obtained all at once after only one round of training (that is, an individual MCMC run to approximate the posterior distribution). As a consequence, what we have in practice is a single algorithm that can produce multiple predictors at the same computational cost, so that any of them can be chosen (or even switched back and forth) depending on the particularities of the problem at hand. Moreover, one could even contemplate an \textit{ensemble model} in which some kind of aggregation of several of the available prediction methods is performed to produce a final result. Also, observe that the choice of a specific point estimator to summarize the posterior distribution results in a veiled assumption of an underlying loss function between the estimated and real parameters. In general, the mean is more sensitive to outliers and the median is more robust, but the latter assumes an \(L^1\)-type loss function while the former implicitly optimizes an \(L^2\) loss. On the other hand, the mode is also a good candidate because it represents the point of highest probability density. At any rate, these decisions are strongly dependent on several factors such as the skewness or the number of modes in the resulting posterior distribution, and thus should be made on a case-by-case basis.

\section{Functional logistic regression}\label{sec:rkhs-logistic-model}

In the case of functional logistic regression, we can regard the binary response variable \(Y\in\{0, 1\}\) as a Bernoulli random variable given the regressor \(X=x \in L^2[0, 1]\), and as suppose that \(\log\left(p(x)/(1-p(x))\right)\) is linear in \(x\), where \(p(x)=\P(Y=1| X=x)\). Then, following the approach  suggested by \citet{berrendero2018functional}, a RKHS model is given  by the  equation
\begin{equation}\label{eq:rkhs-model-logistic}
  \P(Y=1 \mid X) = \frac{1}{1 + \exp\{-\alpha_0 - \Psi_X(\alpha)\}}, \quad \alpha_0 \in \R, \ \alpha \in \Hcal_{0,p}(K).
\end{equation}

Indeed, note that this can be seen as a finite-dimensional approximation (but, still, with a functional interpretation) to the general RKHS functional logistic model proposed by these authors, which can be obtained by replacing \(\Hcal_{0,p}(K)\) with the whole RKHS space \(\Hcal(K)\). Now, if we aim at a classification problem, our strategy will be similar to that followed in the functional linear model: after incorporating the sample information, we can rewrite~\eqref{eq:rkhs-model-logistic} as
\begin{equation}\label{eq:rkhs-model-logistic-2}
Y_i \mid X_i,\theta \ \stackrel{\text{i.i.d.}}{\sim} \operatorname{Bernoulli}(p_i), \quad i=1,\dots, n,
\end{equation}
with
\begin{equation}\label{eq:rkhs-model-logistic-2-parameter}
  p_i = \P(Y_i=1 \mid X_i,\theta) = \frac{1}{\displaystyle 1 + \exp\left\{-\alpha_0 - \sum_{j=1}^p \beta_j X_i(t_j)\right\}}, \quad i=1,\dots, n,
\end{equation}
where in turn \(\beta_j\in\R\) and \(t_j\in[0, 1]\).

In much the same way as the linear regression model described above, this RKHS-based logistic regression model offers some advantages over the \(L^2\)-model. First and foremost, it has a more straightforward interpretation and allows for a workable Bayesian approach, as we will demonstrate below. Secondly, it can be shown that under mild conditions the general RKHS logistic functional model holds whenever the conditional distributions \(X | Y=0\) and \(X|Y=1\) are homoscedastic Gaussian processes \citep[see Theorem 1 in][]{berrendero2018functional}; this provides a sound theoretical motivation for the reduced model. Furthermore, a maximum likelihood approach for parameter estimation (although not considered here) is possible as well. Indeed, the use of a finite-dimensional approximation  mitigates the problem of non-existence of the MLE in the functional case. However, let us recall that even in finite-dimensional settings there are cases of quasi-complete separation in which the MLE does not exist \citep{albert1984existence}, though this issue can be circumvented using, for example, Firth's corrected estimator \citep{firth1993bias}.

\subsection*{The Bayesian approach: prior and posterior}

As far as prior distributions go, we propose to use the same ones as we did in~\eqref{eq:prior-linear} for the linear regression model. However, in this case the nuisance parameter \(\sigma^2\) only appears as part of the hierarchical prior distribution, and not in the final model. The posterior distribution is then derived after a routine calculation.

\begin{proposition}
Under the logistic model~\eqref{eq:rkhs-model-logistic-2}, the prior distribution implied in~\eqref{eq:prior-linear} produces the log-posterior distribution
\begin{align*}
  \log \pi(\theta \mid \D_n) \propto {} & \sum_{i=1}^n \left[ \left(\alpha_0 + \Psi_{X_i}(\alpha)\right)Y_i - \log\left(1 + \exp\left\{\alpha_0 + \Psi_{X_i}(\alpha)\right\}\right)\right]\\
  \quad &+ \frac{1}{2}\log |G_\tau| - (p+2)\log \sigma -\frac{1}{2g\sigma^2} (b - b_0)'G_\tau(b - b_0).
\end{align*}
Remember that \(\Psi_{X_i}(\alpha) = \sum_{j=1}^p \beta_j X_i(t_j)\).
\end{proposition}

\subsection*{Making predictions}

Bear in mind that in this case we are essentially approximating the probabilities \(p_i\) in~\eqref{eq:rkhs-model-logistic-2-parameter}, so before producing a response we need to transform the predicted values to a binary output in \(\{0, 1\}\). According to the usual criterion of minimizing the misclassification probability, it is known that the Bayes optimal rule is recovered by predicting \(\hat Y=1\) whenever \(\P(Y=1|X) \geq 1/2\). Nevertheless, for a more general cost function one could consider other criteria that would lead to evaluating whether \(\P(Y=1|X) \geq \gamma\) for some threshold \(\gamma\in[0, 1]\).

With this last strategy in mind, the summarize-then-predict approach on the approximate posterior distribution is analogous to the linear regression case:
\begin{equation}\label{eq:summarize-predict-logistic}
\hat Y_i = \I \left( \left[\displaystyle 1 + \exp\left\{-\hat\alpha_0 - \sum_{j=1}^p \hat\beta_j X_i(\hat t_j)\right\}\right]^{-1} \geq \gamma \right), \quad i=1,\dots,n',
\end{equation}
where \(\I\) is the indicator function (\(\I(P)\) is \(1\) if \(P\) is true and \(0\) otherwise). The hat estimates are obtained once again through a summary statistic \(T\) of the corresponding marginal posterior distributions.

On the other hand, the prediction method that takes into account the entire posterior approximation (that is, the predict-then-summarize approach) is somewhat different now, since there is the question of which response (the Bernoulli variables in~\eqref{eq:rkhs-model-logistic-2} or the raw probabilities in~\eqref{eq:rkhs-model-logistic-2-parameter}) to consider when averaging the posterior samples. Hence, there are primarily two possible outcomes.

  \paragraph{Average sampled probability.} If we choose to average the approximate probabilities \(p_i^{(m)*} = \P(Y_i =1 | X_i,\theta^{(m)*})\) computed following~\eqref{eq:rkhs-model-logistic-2-parameter}, the resulting predictor is
  \[
    \hat Y_i = \I\left(\frac{1}{M} \sum_{m=1}^M p_i^{(m)*} \geq \gamma\right), \quad i=1,\dots,n'.
  \]
  \paragraph{Average sampled response.} Deciding to average the approximate binary responses \(Y_i^{(m)*}\) instead (see~\eqref{eq:sampled-response-vector}) leads to computing the predictions as
  \[
    \hat Y_i = \I\left(\frac{1}{M} \sum_{m=1}^M Y_i^{(m)*} \geq \gamma\right), \quad i=1,\dots,n'.
  \]
  In this case, each \(Y_i^{(m)*}\) follows a Bernoulli distribution of parameter \(p_i^{(m)*}\), for \(m=1,\dots,M\). Note that when \(\gamma=1/2\) this is equivalent to predicting \(Y_i\) from the majority vote of all the \(Y_i^{(m)*}\).\\

Lastly, the variable selection method is essentially the same as in the case of linear regression: we select \(p\) time instants from each trajectory based on a summary of the posterior distribution \(\tau | \D_n\), and then feed the reduced data set to a finite-dimensional binary classification procedure.
