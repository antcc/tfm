%
% Copyright (c) 2021 Antonio Coín Castro
%
% This work is licensed under a
% Creative Commons Attribution-ShareAlike 4.0 International License.
%
% You should have received a copy of the license along with this
% work. If not, see <http://creativecommons.org/licenses/by-sa/4.0/>.
%

\RequirePackage{fix-cm}
\documentclass[10pt, spanish, professionalfonts]{beamer}

\usepackage{pifont}  % xmark, cmark
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% OPCIONES DE BEAMER

\definecolor{Maroon}{cmyk}{0, 0.87, 0.88, 0.1}
\definecolor{teal}{rgb}{0.0, 0.45, 0.45}

\usetheme[block=fill, subsectionpage=progressbar, titleformat section=smallcaps]{metropolis}
\setbeamertemplate{frametitle continuation}[roman]
\setbeamertemplate{section in toc}[sections numbered]
%\setbeamertemplate{subsection in toc}[subsections unnumbered]
%\setsansfont[BviejoFont={Fira Sans SemiBold}]{Fira Sans Book}  % Increase font weigth
\widowpenalties 1 10000
\raggedbottom

% COLORES
\setbeamercolor{palette primary}{bg=teal}
\setbeamercolor{progress bar}{use=Maroon, fg=Maroon}

% PAQUETES

\usepackage[utf8]{inputenc}
\usepackage[absolute,overlay]{textpos}
\usepackage[spanish, es-nodecimaldot]{babel}
\usepackage{microtype}
\usepackage{epigraph}
\usepackage{hyperref}
\usepackage{bm}
\usepackage{amssymb, amsmath, amsthm, amsfonts, amscd}
\usepackage{listings}

% FONTS
%\usefonttheme{professionalfonts}
%\usepackage{mathpazo}
%\usepackage{eulervm}

\definecolor{backg}{HTML}{F2F2F2} % Fondo
\definecolor{comments}{HTML}{a8a8a8} % Comentarios
\definecolor{keywords}{HTML}{08388c} % Palabras clave
\definecolor{strings}{HTML}{0489B1}  % Strings

\lstset{
language=scala,
basicstyle=\footnotesize\ttfamily,
breaklines=true,
keywordstyle=\color{keywords},
commentstyle=\color{comments},
stringstyle=\color{strings},
xleftmargin=.5cm,
tabsize=2,
% Acentos, ñ, ¿, ¡ (tex.stackexchange.com/questions/24528)
extendedchars=true
}

% COMANDOS PERSONALIZADOS


\renewcommand{\baselinestretch}{1}
\definecolor{mLightBrown}{HTML}{f97e0b}
\newcommand\maroon[1]{\color{mLightBrown}#1\color{black}}
\let\lmin\wedge
\let\lmax\vee
\newtheorem{prop}{Proposición}
\newtheorem{teorema}{Teorema}
\newtheorem{defi}{Definición}
\newcommand\ddfrac[2]{\frac{\displaystyle #1}{\displaystyle #2}}  % Fracción grande

\renewcommand{\epsilon}{\varepsilon}

\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Hcal}{\ensuremath\mathcal{H}}

%% Scalar product
\newcommand\dotprod[2]{\left\langle #1, #2 \right\rangle}

% PLOTS

%\usepackage{pgfplots}
%\DeclareUnicodeCharacter{2212}{−}
%\usepgfplotslibrary{groupplots,dateplot}
%\usetikzlibrary{patterns,shapes.arrows}
%\pgfplotsset{compat=newest}

% TÍTULO

\title{Métodos Bayesianos en modelos RKHS para regresión funcional}
\providecommand{\subtitle}[1]{}
\subtitle{Marco teórico y experimentación en Python}
\date{Congreso SEIO Granada\\ 8 de mayo de 2022\\}
\author{José R. Berrendero \\ Antonio Coín  \\ Antonio Cuevas\\}
\institute{Universidad Autónoma de Madrid \\ \textit{Departamento de Matemáticas}}

\titlegraphic{
  \begin{textblock*}{2cm}(9.8cm, 6.7cm)
    \includegraphics[width=2cm]{img/logo-uam}
  \end{textblock*}
}
% DOCUMENTO

\begin{document}
\maketitle

\begin{frame}{Índice de contenidos}
  \tableofcontents
\end{frame}

\section{TODO}

\begin{frame}{Problemas a resolver}
  \(\bullet\) Regresión \maroon{lineal} con datos funcionales

\begin{figure}
    \includegraphics[width=.65\textwidth]{img/data_lin}
  \end{figure}

\(\bullet\) Regresión \maroon{logística} con datos funcionales

\begin{figure}
    \includegraphics[width=.65\textwidth]{img/data_log}
  \end{figure}
\end{frame}

\begin{frame}{Modelos usuales}
  \(\bullet\) \(Y\) variable real (continua o binaria).

  \(\bullet\) \(X(t)\) proceso estocástico de segundo orden con trayectorias en \(L^2[0, 1]\).

  \vspace{1em}

\begin{block}{Modelo lineal \(\bm{L^2}\)}
  \[
    Y = \alpha_0 + \maroon{\langle X, \beta \rangle_2} + \varepsilon = \alpha_0 + \int_0^1 X(t)\beta(t)\, dt + \varepsilon.
  \]
\end{block}
\begin{block}{Modelo logístico \(\bm{L^2}\)}
    \[
    \mathbb P (Y=1\mid X) = \frac{1}{\exp\{-\alpha_0 - \maroon{\langle X, \beta\rangle_2}\}} = \frac{1}{\exp\{-\alpha_0 - \int_0^1 X(t)\beta(t)\, dt\}}.
  \]
\end{block}
  \(\alpha_0\in\mathbb R\), \(\beta \in L^2[0, 1]\), \(\varepsilon \sim \mathcal N(0, \sigma^2), \mathbb E[\varepsilon]=0, X \bot \varepsilon\).
\end{frame}


\begin{frame}{Algunas desventajas de los modelos \(\bm{L^2}\)}
  \begin{itemize}
    \item \(L^2[0, 1]\) es un espacio extenso, que contiene además funciones muy irregulares.
    \item La teoría usual de mínimos cuadrados no es aplicable directamente. Se requieren técnicas de regularización o reducción de dimensionalidad.
    \item No incluyen como casos particulares modelos sencillos basados en combinaciones lineales de las marginales del proceso, e.g.:
    \[
      Y = \alpha_0 + \sum_{j=1}^p \beta_j X(t_j) + \varepsilon.
    \]
    \item En el caso logístico, no existe el estimador de máxima verosimilitud con probabilidad 1 bajo condiciones muy generales.
  \end{itemize}
\end{frame}


\begin{frame}{Modelo alternativo: RKHS}

Haremos uso de la teoría de los \textit{espacios de Hilbert con núcleo reproductor} (RKHS's, por sus siglas en inglés).

\begin{alertblock}{Propuesta}
  Cambiar el hábitat del parámetro functional \(\beta(\cdot)\).
\end{alertblock}

  En lugar de suponer \(\beta \in L^2[0, 1]\), consideramos \maroon{\(\beta \in \mathcal H(K)\)}, el RKHS asociado a la función de covarianza \(K(t, s)=\mathbb E[X(t)X(s)]\) del proceso centrado \(X\). Además, sustituiremos la evaluación de \(\dotprod{X}{\beta}_2\) por \maroon{\(\dotprod{X}{\beta}_K\)}.
\end{frame}


\begin{frame}{Un repaso muy breve de los RKHS's}
  \begin{definition}
    Suponemos que la función de convarianzas \(K\) (el \textit{kernel}) es continua, y definimos el espacio
    \[
    \Hcal_0(K) = \left\{ f \in L^2[0,1]: \ f(\cdot) = \sum_{i=1}^p a_i K(t_i, \cdot),  p \in \N,  a_i \in \R,  t_i \in [0, 1] \right\},
    \]
    al que dotamos del producto escalar \(\dotprod{f}{g}_K = \sum_{i, j} a_i b_j K(t_i, s_j)\), dadas \(f(\cdot)=\sum_i a_i K(t_i, \cdot) \) y \(g(\cdot)=\sum_j b_j K(s_j, \cdot)\). Entonces, \(\Hcal(K)\) se obtiene al completar \(\Hcal_0(K)\) bajo este producto escalar.
  \end{definition}

  \(\Hcal(K)\subset L^2[0, 1]\), pero las funciones en \(\Hcal(K)\) son más simples y manejables en general. Además, es un espacio de \maroon{funciones}, y no de clases de equivalencia.
\end{frame}


\begin{frame}{Un repaso muy breve de los RKHS's II}
Se pueden dar otras definiciones equivalentes de \(\Hcal(K)\), basadas en el \textit{operador de covarianzas}
\[
\mathcal Kf(\cdot) = \int_0^1 K(s, \cdot)f(s)\, ds, \quad f\in L^2[0, 1].
\]

\begin{definition}
\begin{enumerate}
  \item  \(\Hcal(K) = \mathcal K^{1/2}(L^2[0, 1])\), con \(\dotprod{f}{g}_K = \dotprod{\mathcal K^{-1/2}(f)}{\mathcal K^{-1/2}(g)}_2\).
  \item \(\Hcal(K) = \{f \in L^2[0, 1]: \ \sum_j \lambda_j^{-1}\dotprod{f}{\phi_j}^2 < \infty \}\), con \(\dotprod{f}{g}_K = \sum_j \lambda_j^{-1}\dotprod{f}{\phi_j}\dotprod{g}{\phi_j}_2\). En este caso, \(\lambda_j\) y \(\phi_j\) son los autovalores y autofunciones de \(\mathcal K\).
\end{enumerate}
\end{definition}

La segunda de estas definiciones enfatiza el hecho de que las funciones de \(\Hcal(K)\) son "simples", en el sentido de que sus componentes en una base ortonormal tienden a \(0\) rápidamente.

\end{frame}

\begin{frame}{Una pequeña sutileza}

  En general, las realizaciones \(x=X(\omega)\) \maroon{no pertenecen a \(H(K)\)} \ con probabilidad 1, por lo que la expresión \(\dotprod{x}{\beta}_K\) carece de sentido. Sin embargo, aprovechamos la relación entre \(\Hcal(K)\) y \(\mathcal L(X)\), el espacio de Hilbert generado por el proceso \(X\) en \(L^2(\Omega)\).
  \begin{block}{Isometría de Loève}
    Es la completación de la correspondencia
    \[
\sum_{j=1}^p a_j X(t_j) \longleftrightarrow \sum_{j=1}^p a_j K(t_j, \cdot),
    \]
    Formalmente, \(\Psi^{-1}_X(U)(t) := \E[U X(t)]\) para \(U \in \mathcal L(X)\).
  \end{block}
  Utilizaremos la notación \maroon{\(\dotprod{x}{\beta}_k \equiv \Psi_x(\beta)\)}.
\end{frame}



\begin{frame}{Un nuevo parámetro funcional...}
  \begin{itemize}
    \item La idea inicial era suponer \(\beta \in \Hcal(K)\).
    \item Para simplificar el modelo, haremos la suposición extra de que \(\beta \in \Hcal_0(K)\), que es denso en \(\Hcal(K)\) por construcción. Es decir,
    \[
      \beta(\cdot) = \sum_{j=1}^p \beta_j K(t_j, \cdot).
    \]
    \item A la hora de evaluar \(\dotprod{x}{\beta}_K\) para \(x=X(\omega)\), utilizaremos la isometría de Loève:
    \[
      \dotprod{x}{\beta}_K = \Psi_x(\beta) = \sum_{j=1}^p \beta_j X(t_j).
    \]
  \end{itemize}

\end{frame}


\begin{frame}{...y dos nuevos modelos}
  Dado un conjunto de datos \(\{(X_i, Y_i): i=1,\dots, n\}\) con muestras i.i.d. de \((X, Y)\), consideramos:

  \vspace{1em}

  \begin{block}{Modelo lineal RKHS}
  \[
    Y_i = \alpha_0 + \maroon{\langle X_i, \beta \rangle_K} + \varepsilon = \alpha_0 + \sum_{j=1}^p \beta_j X_i(t_j) + \varepsilon.
  \]
\end{block}
\begin{block}{Modelo logístico RKHS}
    \[
    \mathbb P (Y_i=1\mid X_i) = \frac{1}{\exp\{-\alpha_0 - \maroon{\langle X_i, \beta\rangle_K}\}} = \frac{1}{\exp\{-\alpha_0 - \sum_{j=1}^p \beta_j X_i(t_j)\}}.
  \]
\end{block}
\end{frame}

\begin{frame}{Estimación de parámetros: un enfoque bayesiano}
  teorema de bayes + aproximación posteriori
  se extiende por densidad
  vector de parámetros theta
\end{frame}

\begin{frame}{Distribuciones a priori}
  Se propone utilizar las mismas distribuciones a priori tanto para el caso de regresión lineal como para el de regresión logística:
  \begin{align*}
    p &\sim \text{Categórica}(p_{\text{max}}),\\
  \pi(\alpha_0, \sigma^2)              & \propto 1/\sigma^2,                                                     \\
  \tau                     & \sim \mathcal U([0, 1]^p),                                              \\
  \beta\mid \tau, \sigma^2 & \sim \mathcal N\big(b_0, g\sigma^2{\underbrace{\left[\mathcal X_\tau' \mathcal X_\tau + \eta I\right]}_{G_\tau}}^{-1}\big),
\end{align*}
donde \(I\) es la matriz identidad, \(\mathcal X_\tau = (X_i(t_j))_{i,j}\), y \(b_0\in \R^p, \ g \in \R\) y \(\eta \in \R^+\) son hiperparámetros.

\metroset{block=transparent}
\begin{alertblock}{Selección de hiperparámetros}
  \vspace{0.1em}
  Para \(b_0\) se usa una aproximación computacional del MLE; el resto de hiperparámetros se fija por \textit{cross-validation}.
\end{alertblock}
\metroset{block=fill}
\end{frame}

\begin{frame}{Distribuciones a posteriori}
  \begin{align*}
  \log \pi(\theta\mid \boldsymbol{Y}) &\propto \frac{1}{2}\log |G_\tau| - (p+n)\log \sigma\\
  &-\frac{1}{2\sigma^2} \left(\|\boldsymbol{Y}-\alpha_0\boldsymbol{1} - \mathcal X_\tau\beta\|^2 + \frac{1}{g}(\beta - b_0)'G_\tau(\beta - b_0) \right).
\end{align*}
\end{frame}



\begin{frame}{Predicciones}
  \textbf{Estimación puntual}: Se resume la distribución a posteriori de los parámetros \(\theta\mid Y\) mediante un estimador puntual (media, mediana, moda), y se utilizan para predecir según el modelo:
  \[
  \hat Y_i =\hat \alpha_0 + \sum_{j=1}^p \hat \beta_j x_i(\hat \tau_j), \quad i=1,\dots, n.
  \]

  \textbf{Estimación distribucional}: Se utiliza la media de \textit{todas}  las muestras generadas de \(Y^*\) como predicción:
  \[
    \hat Y = \frac{1}{M}\sum_{m=1}^M Y^{(j)*}.
  \]

  En ambos casos se puede considerar solo una de cada \(k\) muestras.
\end{frame}

\begin{frame}{Predicciones II}
    Podemos hacer también una selección de \(p\) variables usando los estimadores puntuales de \(\tau \mid Y\), y después aplicar cualquier algoritmo de regresión lineal.

    \begin{figure}
      \includegraphics[width=\textwidth]{img/tau_posterior_lin}
      \caption{Distribución a posteriori estimada para los puntos de impacto.}
    \end{figure}

    En cualquier caso, medimos el error usando el MSE como métrica.
\end{frame}

\begin{frame}{Model checking}
  \begin{itemize}
    \item Análisis de la traza de las cadenas y de la distribución a posteriori de los parámetros. Se obtienen \textbf{intervalos creíbles} para los parámetros.
    \item En cada paso obtenemos una estimación \(\tilde \theta_m\), y podemos generar \(Y^{(m)*} \mid \theta_m, X\) siguiendo el modelo asumido.
    \item \textit{Bayesian p-values}: \(p=P(T(Y^*)\leq T(Y)\mid Y)\) para ciertas elecciones de \(T\): mínimo, máximo, mediana, media. Se calcula contando la proporción de muestras generadas que cumplen la desigualdad, y se espera que esté en torno a \(0.5\).
  \end{itemize}

\end{frame}

\begin{frame}{Model checking II}
  \begin{figure}
    \includegraphics[width=\textwidth]{img/ppc_lin}
    \caption{\textit{Posterior predictive checks} bajo un modelo RKHS.}
  \end{figure}
\end{frame}

\subsection{Experimentos}

\begin{frame}{Datos sintéticos}
  Se consideran 150 ejemplos de \(X \sim GP(0, K(s, t))\) y tres posibles variantes de \(K\): movimiento browniano fraccional (\(H=0.8\)), Ornstein-Uhlenbeck y kernel RBF.

  Se genera la respuesta \(Y\) acorde a dos modelos, RKHS y \(L^2\). Concretamente, se elige
  \[
    Y_i \sim \mathcal N\big(5 -5X_i(0.1) + 10X_i(0.8), \ 0.5\big)
  \]
  ó
  \[
    Y_i \sim \mathcal N\left(5 + \int_0^1 \log(1+4t)X_i(t)\, dt, \ 0.5\right).
  \]

Consideramos una malla regular de \(N=100\) puntos en \([0, 1]\), y un reparto de 100 ejemplos para entrenamiento y 50 para evaluación.
\end{frame}

\begin{frame}{Datos reales}
  Se consideran dos conjuntos de datos reales.
  \begin{itemize}
    \item \textbf{Tecator:} Contiene 215 ejemplos de mediciones de \textit{absorbancia} en muestras de carne para intentar predecir su contenido en grasa.
    \item \textbf{Aemet}: Contiene 73 ejemplos de curvas de temperatura, a partir de las cuales se intenta predecir la precipitación total.
  \end{itemize}

  En ambos casos hacemos una división \(80\%-20\%\) para entrenamiento y \textit{test}.
\end{frame}

\begin{frame}{Preprocesado de los datos}
  \begin{itemize}
    \item Se centran los regresores para que tengan media \(0\). Opcionalmente, se pueden estandarizar en cada punto de la malla.
    \item Se permite \textbf{sustituir los datos \(\boldsymbol{X_i}\) por su desarrollo en una base} de Fourier, con un número determinado de coeficientes. De esta forma realizamos un suavizado de las curvas.
  \end{itemize}
\end{frame}

\begin{frame}{Algoritmos de comparación}
  \textbf{Regresión lineal multivariante:} Regresión Lineal estándar, Lasso (\(L^1\)), Ridge (\(L^2\)).

  \textbf{Regresión no lineal multivariante}: SVM con kernel RBF.

  \textbf{Regresión lineal funcional}: Modelo \(L^2\), KNN Funcional.

  \textbf{Reducción de dimensión:} PCA Funcional (proyección sobre coeficientes).

  \textbf{Selección de variables:} Aleatoria.

  \vspace{1em}

  Se entrenan todos ellos sobre el conjunto de entrenamiento, haciendo \textit{5-fold cross validation} para escoger los mejores hiperparámetros en cada caso (valor de regularización, número de componentes, ...).
\end{frame}

\begin{frame}{Metodología experimental}
  \begin{itemize}
  \item Para cada conjunto de datos consideramos tres posibles valores de \(p\) y cuatro posibles valores de \(\eta\):
  \begin{itemize}
    \item[--] \(p\in \{2,3,4\}\) para los datos sintéticos y \(p\in \{3, 4, 5\}\) para los datos reales.
      \item[--] \(\eta \in \{0.01, 0.1, 1.0, 10.0\}\).
  \end{itemize}

  \item Para aumentar la estabilidad hacemos 5 repeticiones de cada modelo. Es decir, entrenamos un total de 60 modelos sobre el conjunto de entrenamiento

  \item Con el \textbf{mejor modelo obtenido} se realiza también una selección de variables basada en los valores de \(\tau\) estimados mediante los distintos estimadores puntuales.
\end{itemize}

Además, repetimos este proceso con los datos suavizados en una base de Fourier con 11 elementos (también con los algoritmos de comparación).
\end{frame}

\begin{frame}{Metodología experimental II}
  Fijamos algunos hiperparámetros:
  \begin{itemize}
    \item Número de cadenas: 64.
    \item Número de pasos: 100 + 1000.
    \item Movimientos: elección aleatoria (ponderada) entre dos de los recomendados, \textit{stretch} y \textit{walk}.
    \item Inicialización: Entorno aleatorio del MLE + muestras de las distribuciones a priori.
    \item Prior de \(\beta\): \(b_0\) se elige como el MLE de \(\beta\), y fijamos \(g=5\) (recomendado en \textit{Grollemund et al. (2019)}).
  \end{itemize}
  \vspace{1em}

  \textit{Nota:} El valor de \(g\) no parece afectar al resultado, por lo que podríamos eliminar este parámetro.
\end{frame}

\begin{frame}{Resultados RKHS}
\textbf{Sin base}
  \begin{table}
    \begin{tabular}{c|cc}
      Kernel & Victoria & Victoria sel. variables \\ \hline
      fBM & \cmark & \cmark\\
      O-U & \cmark & \cmark\\
      RBF & \cmark & \cmark
    \end{tabular}
  \end{table}

  \textbf{Base Fourier(11)}
  \begin{table}
    \begin{tabular}{c|cc}
      Kernel & Victoria & Victoria sel. variables \\ \hline
      fBM & \cmark &  \cmark\\
      O-U & \cmark & \xmark\\
      RBF & \cmark & \cmark
    \end{tabular}
  \end{table}
\end{frame}

\begin{frame}{Resultados RKHS II}
  \begin{figure}
    \includegraphics[width=\textwidth]{img/res/reg_emcee_rkhs_fixed}
    \caption{caption.}
  \end{figure}
\end{frame}

\begin{frame}{Resultados RKHS III}
  \begin{figure}
    \includegraphics[width=0.3\textwidth]{img/results/lin_rkhs_fbm_base11}\hfill
    \includegraphics[width=0.3\textwidth]{img/results/lin_rkhs_ou_base11}\hfill
    \includegraphics[width=0.3\textwidth]{img/results/lin_rkhs_rbf_base11}
    \caption{MSE de los 5 mejores modelos en cada caso con el modelo subyacente RKHS y con base de Fourier.}
  \end{figure}
\end{frame}

\begin{frame}{Resultados \(\boldsymbol{L^2}\)}
\textbf{Sin base}
  \begin{table}
    \begin{tabular}{c|cc}
      Kernel & Victoria & Victoria sel. variables \\ \hline
      fBM & \xmark & \xmark\\
      O-U & \xmark & \xmark\\
      RBF & \xmark & \xmark
    \end{tabular}
  \end{table}
\textbf{Base Fourier(11)}
  \begin{table}
    \begin{tabular}{c|cc}
      Kernel & Victoria & Victoria sel. variables \\ \hline
      fBM & \cmark &  \cmark\\
      O-U & \cmark & \cmark\\
      RBF & \xmark & \xmark
    \end{tabular}
  \end{table}
\end{frame}

\begin{frame}{Resultados \(\boldsymbol{L^2}\) II}
  \begin{figure}
    \includegraphics[width=0.3\textwidth]{img/results/lin_l2_fbm_nobase}\hfill
    \includegraphics[width=0.3\textwidth]{img/results/lin_l2_ou_nobase}\hfill
    \includegraphics[width=0.3\textwidth]{img/results/lin_l2_rbf_nobase}
    \caption{MSE de los 5 mejores modelos en cada caso con el modelo subyacente \(L^2\) y sin usar suavizado con bases.}
  \end{figure}
\end{frame}

\begin{frame}{Resultados \(\boldsymbol{L^2}\) III}
  \begin{figure}
    \includegraphics[width=0.3\textwidth]{img/results/lin_l2_fbm_base11}\hfill
    \includegraphics[width=0.3\textwidth]{img/results/lin_l2_ou_base11}\hfill
    \includegraphics[width=0.3\textwidth]{img/results/lin_l2_rbf_base11}
    \caption{MSE de los 5 mejores modelos en cada caso con el modelo subyacente \(L^2\) y con base de Fourier.}
  \end{figure}
\end{frame}

\begin{frame}{Resultados datos reales}
\textbf{Sin base}
  \begin{table}
    \begin{tabular}{c|cc}
      Dataset & Victoria & Victoria sel. variables \\ \hline
      Tecator & \xmark & \xmark\\
      Aemet & \xmark & \cmark\\
    \end{tabular}
  \end{table}

  \textbf{Base Fourier(11)}
  \begin{table}
    \begin{tabular}{c|cc}
      Dataset & Victoria & Victoria sel. variables \\ \hline
      Tecator & \xmark &  \xmark\\
      Aemet & \xmark & \cmark\\
    \end{tabular}
  \end{table}
\end{frame}

\begin{frame}{Resultados datos reales II}
  \begin{figure}
    \includegraphics[width=0.3\textwidth]{img/results/lin_tecator_nobase}\hspace{5em}
    \includegraphics[width=0.3\textwidth]{img/results/lin_aemet_nobase}
    \caption{MSE de los 5 mejores modelos en cada caso con datos reales y sin usar suavizado con bases.}
  \end{figure}
\end{frame}

\begin{frame}{Resultados datos reales III}
  \begin{figure}
    \includegraphics[width=0.3\textwidth]{img/results/lin_tecator_base11}\hspace{5em}
    \includegraphics[width=0.3\textwidth]{img/results/lin_aemet_base11}
    \caption{MSE de los 5 mejores modelos en cada caso con datos reales y con base de Fourier.}
  \end{figure}
\end{frame}

\begin{frame}{Algunas observaciones}
\begin{itemize}
  \item Por cada modelo entrenado obtenemos 4 estimadores y 3 estrategias de selección de variables.
  \item En general, se obtienen mejores resultados con el uso de la base.
  \item El tiempo medio de entrenamiento de cada modelo es de unos 20-30 segundos.
  \item En ocasiones, nuestro algoritmo supera con holgura a todos los algoritmos de comparación. Cuando pierde, suele ser frente a uno o dos de ellos (y por poco), pero sigue superando al resto.
  \item Se puede considerar como algoritmo de comparación el estimador de máxima verosimilitud de los parámetros. En casi todas las pruebas experimentales, el desempeño de nuestro algoritmo supera al del MLE.
\end{itemize}


\end{frame}

\begin{frame}{Dificultades}
  \begin{itemize}


  \item Hay multitud de grados de libertad: estandarización de regresores y/o variable respuesta, elección o no de una base (¿cuál?,) algoritmo de estimación del MLE, número, longitud y movimientos de las cadenas MCMC, elección de distribuciones a priori, elección de \(g, b_0\) y \(\eta\), etc.
  \item El algoritmo MCMC es costoso, y en conjuntos de datos grandes las estrategias de \textit{cross-validation} para seleccionar parámetros pueden no ser viables.
  \item Es necesario un procedimiento para seleccionar el valor de \(p\) (BIC, WAIC, WBIC, \textit{cross-validation}, ...).
  \item Debido a la aleatoriedad del algoritmo, los resultados pueden variar sustancialmente de una ejecución a otra.
  \item La distribución a priori para \(\beta\) depende en gran medida del valor de \(b_0\) escogido. Si su estimación inicial no es buena, el algoritmo no funciona demasiado bien.
\end{itemize}
\end{frame}

\begin{frame}{Dificultades II}
  \begin{itemize}
    \item Hay un problema de \textit{identificabilidad} de los coeficientes (son intercambiables), que se agudiza especialmente al usar varias cadenas.
    \item En el caso en el que el modelo subyacente es RKHS, es posible que no se recuperen los verdaderos valores de los parámetros debido a interacciones entre los coeficientes (si \(p\) es mayor que el número real de componentes).
    \item El uso de distribuciones a priori impropias implica comprobar que \(\int_{\Theta} \pi(Y\mid \theta)\pi(\theta)\, d\theta < \infty\).
  \end{itemize}
\end{frame}

\begin{frame}{Alternativas}
  \begin{itemize}
    \item Explorar el concepto de \textit{online learning} aplicado a esta situación, por ejemplo usando como priori de nuevos datos la posteriori aprendida. Estudiar la \textbf{consistencia} de la distribución a posteriori.
    \item Utilizar otras herramientas de suavizado en lugar de bases de Fourier.
    \item Sustitutir la distribución a priori de \(\beta\) por otra que requiera menos hiperparámetros.
    \item Sustituir las distribuciones impropias por otras propias.
  \end{itemize}

\end{frame}

\section{Regresión Logística Funcional Bayesiana}

\subsection{Marco teórico}

\begin{frame}{Planteamiento Bayesiano}
\textbf{Modelo}: Cada \(Y_i\) se puede ver como una variable aleatoria de Bernoulli \(\mathcal B(p(x_i))\), con
\[
p_i \equiv p(x_i)=\mathbb P(Y_i=1\mid X_i=x_i) = \frac{1}{1 + \exp\left\{-\alpha_0-\displaystyle\sum_{j=1}^p \beta_jx_i(\tau_j)\right\}}.
\]

\textbf{Distribuciones a priori}: Igual que en regresión lineal.

\textbf{Log-posterior}:
\begin{align*}
  \log \ &\pi(\beta, \tau, \alpha_0, \log\sigma\mid Y) \propto\\
   &\sum_{i=1}^n \left[ \left(\alpha_0 + \Psi^{-1}_{x_i}(\alpha)\right)y_i - \log\left(1 + \exp\left\{\alpha_0 + \Psi_{x_i}^{-1}(\alpha)\right\}\right)\right]\\
   &+
\frac{1}{2}\log |G_\tau| - p\log \sigma -\frac{1}{2g\sigma^2} (\beta - b_0)'G_\tau(\beta - b_0).
\end{align*}
\end{frame}

\begin{frame}{Model checking}
  \begin{figure}
    \includegraphics[width=\textwidth]{img/ppc_log}
    \caption{\textit{Posterior predictive checks} bajo un modelo RKHS.}
  \end{figure}
\end{frame}

\begin{frame}{Predicciones}

\begin{itemize}
  \item Fijamos un umbral en \(0.5\) para establecer la pertenencia a las clases.
   \item Los estimadores puntuales y la selección de variables son análogos al caso de regresión lineal.
   \item Tenemos ahora dos estimadores basados en la distribución a posteriori (similares a los \textit{ensembles} de clasificadores):
   \begin{itemize}
     \item[--] Basado en el voto mayoritario de las muestras \(Y^*\) generadas.
     \item[--] Basado en la media de las probabilidades \(p_i^*\) generadas (con aplicación posterior del umbral).
   \end{itemize}
  \item Se usa el \textit{accuracy} (precisión) para evaluar los modelos.
\end{itemize}
\end{frame}

\subsection{Experimentos}

\begin{frame}{Datos sintéticos}
Seguimos la misma estrategia de generación de datos antes. Se consideran las mismas tres funciones de covarianza y se genera la respuesta tanto con un modelo RKHS como L2, i.e.:
  \[
    Y_i \sim \mathcal B\left(\frac{1}{1 + \exp\left\{0.5 + 5X_i(0.1) - 10X_i(0.8)\right\}}\right)
  \]
  ó
  \[
    Y_i \sim \mathcal B\left(\frac{1}{1 + \exp\left\{0.5 -\int_0^1 \log(1+4t)X_i(t)\, dt\right\}}\right).
  \]

  \vspace{1em}

  Se introduce un pequeño ruido aleatorio en las etiquetas, y además se intenta que ambas clases estén balanceadas.

\end{frame}

\begin{frame}{Datos reales}
  Se consideran dos conjuntos de datos reales.
  \begin{itemize}
    \item \textbf{Medflies:} Contiene 534 ejemplos de mediciones del número de huevos diario puesto por una serie de moscas, para intentar predecir si viven mucho o poco.
    \item \textbf{Growth}: Contiene 93 ejemplos de curvas de altura en niños y niñas.
  \end{itemize}

  En ambos casos hacemos una división \(80\%-20\%\) para entrenamiento y \textit{test}.
\end{frame}

\begin{frame}{Algoritmos de comparación}
  \textbf{Clasificación lineal multivariante:} Regresión Logística, SVM lineal.

  \textbf{Clasificación no lineal multivariante}: SVM con kernel RBF.

  \textbf{Clasificación funcional}: \textit{Maximum Depth}, \textit{Nearest Centroid} Funcional, KNN Funcional.

  \textbf{Reducción de dimensión:} PCA Funcional (proyección sobre coeficientes).

  \textbf{Selección de variables:} Aleatoria, \textit{Recursive Maxima Hunting}, \textit{RKVS} (Mahalanobis).

  \vspace{1em}

  Mismo preprocesado y metodología experimental que en el caso de regresión lineal, utilizando esta vez 9 coeficientes de Fourier.
\end{frame}

\begin{frame}{Resultados RKHS}
\textbf{Sin base}
  \begin{table}
    \begin{tabular}{c|cc}
      Kernel & Victoria & Victoria sel. variables \\ \hline
      fBM & \xmark & \xmark\\
      O-U & \xmark & \cmark\\
      RBF & \cmark & \xmark
    \end{tabular}
  \end{table}

  \textbf{Base Fourier(9)}
  \begin{table}
    \begin{tabular}{c|cc}
      Kernel & Victoria & Victoria sel. variables \\ \hline
      fBM & \cmark &  \cmark\\
      O-U & \cmark & \cmark\\
      RBF & \cmark & \cmark
    \end{tabular}
  \end{table}
\end{frame}

\begin{frame}{Resultados RKHS II}
  \begin{figure}
    \includegraphics[width=0.3\textwidth]{img/results/log_rkhs_fbm_nobase}\hfill
    \includegraphics[width=0.3\textwidth]{img/results/log_rkhs_ou_nobase}\hfill
    \includegraphics[width=0.3\textwidth]{img/results/log_rkhs_rbf_nobase}
    \caption{Precisión de los 5 mejores modelos en cada caso con el modelo subyacente RKHS y sin usar suavizado con bases. Distinguimos los algoritmos de comparación, nuestro algoritmo Bayesiano (emcee), y nuestra selección de variables Bayesiana.}
  \end{figure}
\end{frame}

\begin{frame}{Resultados RKHS III}
  \begin{figure}
    \includegraphics[width=0.3\textwidth]{img/results/log_rkhs_fbm_base9}\hfill
    \includegraphics[width=0.3\textwidth]{img/results/log_rkhs_ou_base9}\hfill
    \includegraphics[width=0.3\textwidth]{img/results/log_rkhs_rbf_base9}
    \caption{Precisión de los 5 mejores modelos en cada caso con el modelo subyacente RKHS y con base de Fourier.}
  \end{figure}
\end{frame}

\begin{frame}{Resultados \(\boldsymbol{L^2}\)}
\textbf{Sin base}
  \begin{table}
    \begin{tabular}{c|cc}
      Kernel & Victoria & Victoria sel. variables \\ \hline
      fBM & \xmark & \xmark\\
      O-U & \cmark & \xmark\\
      RBF & \cmark & \cmark
    \end{tabular}
  \end{table}

  \textbf{Base Fourier(9)}
  \begin{table}
    \begin{tabular}{c|cc}
      Kernel & Victoria & Victoria sel. variables \\ \hline
      fBM & \xmark &  \cmark\\
      O-U & \xmark & \cmark\\
      RBF & \cmark & \cmark
    \end{tabular}
  \end{table}
\end{frame}

\begin{frame}{Resultados \(L^2\) II}
  \begin{figure}
    \includegraphics[width=0.3\textwidth]{img/results/log_l2_fbm_nobase}\hfill
    \includegraphics[width=0.3\textwidth]{img/results/log_l2_ou_nobase}\hfill
    \includegraphics[width=0.3\textwidth]{img/results/log_l2_rbf_nobase}
    \caption{Precisión de los 5 mejores modelos en cada caso con el modelo subyacente \(L^2\) y sin usar suavizado con bases.}
  \end{figure}
\end{frame}

\begin{frame}{Resultados \(L^2\) III}
  \begin{figure}
    \includegraphics[width=0.3\textwidth]{img/results/log_l2_fbm_base9}\hfill
    \includegraphics[width=0.3\textwidth]{img/results/log_l2_ou_base9}\hfill
    \includegraphics[width=0.3\textwidth]{img/results/log_l2_rbf_base9}
    \caption{Precisión de los 5 mejores modelos en cada caso con el modelo subyacente \(L^2\) y con base de Fourier.}
  \end{figure}
\end{frame}

\begin{frame}{Resultados datos reales}
\textbf{Sin base}
  \begin{table}
    \begin{tabular}{c|cc}
      Dataset & Victoria & Victoria sel. variables \\ \hline
      Medflies & \xmark & \xmark\\
      Growth & \cmark & \cmark\\
    \end{tabular}
  \end{table}

  \textbf{Base Fourier(9)}
  \begin{table}
    \begin{tabular}{c|cc}
      Dataset & Victoria & Victoria sel. variables \\ \hline
      Medflies & \xmark &  \xmark\\
      Growth & \cmark & \cmark\\
    \end{tabular}
  \end{table}

  Tanto en las victorias como en las derrotas, la precisión es muy similar. Podemos considerar un empate en ambos conjuntos.
\end{frame}

\begin{frame}{Resultados datos reales II}
  \begin{figure}
    \includegraphics[width=0.3\textwidth]{img/results/log_medflies_nobase}\hspace{5em}
    \includegraphics[width=0.3\textwidth]{img/results/log_growth_nobase}
    \caption{Precisión de los 5 mejores modelos en cada caso con datos reales y sin usar suavizado con bases.}
  \end{figure}
\end{frame}

\begin{frame}{Resultados datos reales III}
  \begin{figure}
    \includegraphics[width=0.3\textwidth]{img/results/log_medflies_base9}\hspace{5em}
    \includegraphics[width=0.3\textwidth]{img/results/log_growth_base9}
    \caption{Precisión de los 5 mejores modelos en cada caso con datos reales y con base de Fourier.}
  \end{figure}
\end{frame}

\end{document}
